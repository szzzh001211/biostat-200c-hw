---
title: "Midterm Make-up Questions"
author: "Ziheng Zhang (UID 606300061)"
subtitle: Biostat 200C
date: today
format:
  html:
    theme: cosmo
    embed-resources: true
    number-sections: true
    toc: true
    toc-depth: 4
    toc-location: left
    code-fold: false
engine: knitr
knitr:
  opts_chunk: 
    fig.align: 'center'
    # fig.width: 6
    # fig.height: 4
    message: FALSE
    cache: false
---
# Count Data Example - Microbiome (60 pts)

## Introduction
- This data package contains the information used to run the analyses found in "[Diarrhea in young children from low-income countries leads to large-scale alterations in
intestinal microbiota composition](https://genomebiology.biomedcentral.com/articles/10.1186/gb-2014-15-6-r76)". 

- Measurements are the number of reads annotated for a particular cluster within a given sample followed by filtering. Sequencing was performed on the 454 Flex platform. 

- Data is stored as an `MRexperiment-class` object. The count matrix was generated using DNAclust (http://dnaclust.sourceforge.net/). For more details please refer to the paper. Included in the `MRexperiment` object are the counts, phenotype, and feature information.

## Microbiome data background 
- The human microbiome is the collection of all the microorganisms living in association with the human body. These communities consist of a variety of microorganisms including eukaryotes, archaea, bacteria, and viruses.

- The human body, consisting of about 10 trillion cells, carries about ten times as many microorganisms in the intestines. The metabolic activities performed by these bacteria resemble those of an organ, leading some to liken gut bacteria to a "forgotten" organ.

- Hight-throughput sequencing technologies have enabled the study of the human microbiome through the use of metagenomic data.

  + OTU (Operational Taxonomic Unit) is a cluster of sequences that are at least 97% similar. OTUs are used to classify sequences into groups that are similar to each other.
  + OTU counts are used to represent the abundance of each OTU in each sample.
  + OTU counts can be aggregated at higher phylogentic levels, e.g., species, genus, etc.
  + The higher the level the less zero-inflated the data is.

- Features of microbiome data include

  + Zero-inflated: many OTUs are not observed in many samples
  + Overdispersed: the variance is larger than the mean
  + Compositional: the sum of all OTUs in a sample is constant
  + High-dimensional: large number of OTUs

## Goal of this excercise
- One goal of microbiome data analysis is to identify the association between microbiome composition and clinical outcomes. In this study, it is case-control status: diarrhea vs non-diarrhea.

- Try the techniques we learned so far to analyze the microbiome data, at genus and species levels, to identify the association between microbiome composition and case-control status. Use the following models:

  + Quasi-Poisson
  + Negative Binomial
  + Zero Inflated Poisson
  + Zero Inflated Negative Binomial
  
```{r}
# if (!require("BiocManager", quietly = TRUE))
#    install.packages("BiocManager")
# BiocManager::install("msd16s")

# BiocManager::install("metagenomeSeq")
suppressMessages(library(metagenomeSeq))
library(msd16s)
library(tidyverse)
library(broom)
```

### Load data and process
```{r, eval=FALSE}
data(msd16s)
msd16s
```
The phenotype information can be accessed with the phenoData and pData methods:
```{r}
phenoData(msd16s)

pheno_tbl <- rownames_to_column(pData(msd16s), var = "ID") %>% as_tibble()
pheno_tbl

p_case_control = pheno_tbl %>% 
  count(Type) %>% 
  mutate(prop = n/sum(n))

p_case <- with(p_case_control, prop[Type == "Case"])
p_control <- with(p_case_control, prop[Type == "Control"])
```
- The feature information including cluster representative (e.g., OTUs) sequence can be accessed
with the featureData and fData methods:
```{r}
featureData(msd16s)
features <- fData(msd16s)
```

```{r}
counts <- MRcounts(msd16s, norm = TRUE)
dim(counts)
```
The raw or normalized counts matrix can be accessed with the MRcounts function. We get normalized counts. A normalization method avoids biases due to uneven sequencing depth. 

```{r}
otu_id <- rownames(counts)
counts_tbl <- bind_cols(otu_id = otu_id, counts %>% as_tibble())
counts_tbl
```

#### Filter data at OTU level
- OTU was abundant (≥12 normalized counts per sample) in cases or controls; 
```{r}
control_to_select <- pheno_tbl %>% 
  filter(Type == "Control") %>% 
  dplyr::select(ID) %>% 
  pull()

con_sum <- counts_tbl %>%
  dplyr::select(all_of(control_to_select)) %>% 
  rowSums() 

case_to_select <- pheno_tbl %>% 
  filter(Type == "Case") %>% 
  dplyr::select(ID) %>% 
  pull()

case_sum <- counts_tbl %>%
  dplyr::select(all_of(case_to_select)) %>% 
  rowSums() 

counts_tbl_filt <- bind_cols(counts_tbl, 
                             con_sum = con_sum/length(control_to_select), 
                             case_sum = case_sum/length(case_to_select)) %>%
  filter(con_sum >= 12 | case_sum >= 12) 

```

- OTU was prevalent (present in ≥10 cases and controls);
```{r}
otu_prevalence <- counts_tbl_filt %>%
  dplyr:: select(-otu_id) %>%
  mutate(across(everything(), ~ as.integer(. > 0))) %>%
  rowSums()
  
counts_tbl <- bind_cols(counts_tbl_filt, otu_prevalence = otu_prevalence) %>%
  filter(otu_prevalence >= 10) %>%
  dplyr:: select(-con_sum, -case_sum, -otu_prevalence)

```

```{r}
counts_tbl_t <- counts_tbl %>% 
  pivot_longer(cols= -1) %>% 
  pivot_wider(names_from = "otu_id",values_from = "value") %>%
  rename(ID = name) %>% 
  left_join(pheno_tbl, by = "ID") 
```

#### Filter data at Genus level
- Aggregated counts at genus level

```{r}
genus_counts = aggTax(msd16s, lvl = "genus", out = "matrix", norm = T)
genus_id <- rownames(genus_counts)
genus_counts_tbl <- bind_cols(genus_id = genus_id, genus_counts %>% as_tibble())
genus_counts_tbl
```

- Genus was abundant (≥12 normalized counts per sample) in cases or controls; 
```{r}
con_mean <- genus_counts_tbl %>%
  dplyr:: select(all_of(control_to_select)) %>% 
  rowMeans() 

case_mean <- genus_counts_tbl %>%
  dplyr:: select(all_of(case_to_select)) %>% 
  rowMeans() 

genus_tbl_filt <- bind_cols(genus_counts_tbl, 
                             con_mean = con_mean, 
                             case_mean = case_mean) %>%
  filter(con_mean >= 12 | case_mean >= 12) 

```

- Genus was prevalent (present in ≥10 cases and controls);
```{r}
otu_prevalence <- genus_tbl_filt %>%
  dplyr:: select(-genus_id) %>%
  mutate(across(everything(), ~ as.integer(. > 0))) %>%
  rowSums()
  
genus_tbl <- bind_cols(genus_tbl_filt, otu_prevalence = otu_prevalence) %>%
  filter(otu_prevalence >= 10) %>%
  dplyr:: select(-con_mean, -case_mean, -otu_prevalence) %>% 
  filter(genus_id != "NA")

rm(genus_tbl_filt)

genusnames = genus_tbl$genus_id 

```

- Create a tidy table with pheno information
```{r}
genus_tbl_t <- genus_tbl %>% 
  pivot_longer(cols= -1) %>% 
  pivot_wider(names_from = "genus_id",values_from = "value") %>%
  rename(ID = name) %>% 
  left_join(pheno_tbl, by = "ID")  # %>%
  #dplyr::select(-`NA`)
```


#### Filter data at Species level
- Aggregated counts at genus level
```{r}
species_counts = aggTax(msd16s, lvl = "species", out = "matrix", norm = T)
species_id <- rownames(species_counts)
species_counts_tbl <- bind_cols(species_id = species_id, species_counts %>% as_tibble())
species_counts_tbl
```
- Species was abundant (≥12 normalized counts per sample) in cases or controls; 
```{r}
con_mean <- species_counts_tbl %>%
  dplyr:: select(all_of(control_to_select)) %>% 
  rowMeans() 

case_mean <- species_counts_tbl %>%
  dplyr:: select(all_of(case_to_select)) %>% 
  rowMeans() 

species_tbl_filt <- bind_cols(species_counts_tbl, 
                             con_mean = con_mean, 
                             case_mean = case_mean) %>%
  filter(con_mean >= 12 | case_mean >= 12) 

```

- Genus was prevalent (present in ≥10 cases and controls);
```{r}
otu_prevalence <- species_tbl_filt %>%
  dplyr:: select(-species_id) %>%
  mutate(across(everything(), ~ as.integer(. > 0))) %>%
  rowSums()
  
species_tbl <- bind_cols(species_tbl_filt, otu_prevalence = otu_prevalence) %>%
  filter(otu_prevalence >= 10) %>%
  dplyr:: select(-con_mean, -case_mean, -otu_prevalence)

rm(species_tbl_filt)
speciesnames = species_tbl$species_id[-1*which(species_tbl$species_id == "NA")]
```

- Create a tidy table with pheno information
```{r}
species_tbl_t <- species_tbl %>% 
  pivot_longer(cols= -1) %>% 
  pivot_wider(names_from = "species_id",values_from = "value") %>%
  rename(ID = name) %>% 
  left_join(pheno_tbl, by = "ID") %>%
  dplyr:: select(-`NA`)
```

### Question 1

- Compare the results from these models and discuss the pros and cons of each model.

  + Create a table to summarize the significant microbiome clusters from these models.

#### Genus level
- Quasipoisson regression model
```{r}
results_genus_qpoisson <- 
  map_df(genusnames, function(response) {
    model <- glm(as.formula(paste(response, "~ Type")), 
                 data = genus_tbl_t, 
                 family = quasipoisson)
    tidy(model, conf.int = TRUE) %>%
      # Add a column for the response variable
      mutate(response_variable = response)
    }) %>% 
  filter(term == "TypeControl") %>% 
  arrange(p.value) 

genus_qpoisson <- results_genus_qpoisson %>% filter(p.value < 0.05/41) %>%
  dplyr::select(response_variable, p.value) %>%
  print(n = 20)
```
- Negative binomial regression model
```{r}
library(MASS)


results_genus_nb2 <- 
  map_df(genusnames, function(response) {
  #  print(response)
    model <- glm(as.formula(paste(response, "~ Type")), 
                 family = negative.binomial(20),
                 data = genus_tbl_t)
    tidy(model, conf.int = TRUE) %>%
      # Add a column for the response variable
      mutate(response_variable = response)
    }) %>% 
  filter(term == "TypeControl") %>% 
  arrange(p.value) 

genus_nb2 <- results_genus_nb2 %>% filter(p.value < 0.05/41) %>%
  dplyr::select(response_variable, p.value) %>%
  print(n = Inf)
```
- Zero inflated poisson
```{r}
library(pscl)
results_genus_zpoisson <- 
  map_df(genusnames, function(response) {
    genus_tbl_t[[response]] <- round(genus_tbl_t[[response]])
    model <- zeroinfl(as.formula(paste(response, "~ Type")), 
                      data = genus_tbl_t, dist = "poisson")
    coefs <- summary(model)$coefficients$count
    coefs1 <- summary(model)$coefficients$zero
    p_values <- data.frame(
      term = rownames(coefs),
      p.value = coefs[, "Pr(>|z|)"],
      p.value_zero = coefs1[2, "Pr(>|z|)"],
      response_variable = response
    )
    p_values
  }) %>% 
  filter(term == "TypeControl") %>% 
  arrange(p.value) 

genus_zpoisson <- results_genus_zpoisson %>% filter(p.value < 0.05/41) %>%
  filter(p.value_zero < 0.05/41) %>%
  dplyr::select(response_variable, p.value, p.value_zero) %>%
  print(n = 20)
```

- Zero inflated negative binomial
```{r}
library(pscl)
results_genus_znegbin <- 
  map_df(genusnames, function(response) {
    genus_tbl_t[[response]] <- round(genus_tbl_t[[response]])
    model <- zeroinfl(as.formula(paste(response, "~ Type")), 
                      data = genus_tbl_t, dist = "negbin")
    coefs <- summary(model)$coefficients$count
    coefs1 <- summary(model)$coefficients$zero
    p_values <- data.frame(
      term = rownames(coefs),
      p.value = coefs[2, "Pr(>|z|)"],
      p.value_zero = coefs1[2, "Pr(>|z|)"],
      response_variable = response
    )
    
    p_values
  }) %>% 
  filter(term == "TypeControl") %>%
  arrange(p.value)

genus_znegbin <- results_genus_znegbin %>% filter(p.value < 0.05/41) %>%
  filter(p.value_zero < 0.05/41) %>%
  dplyr::select(response_variable, p.value, p.value_zero) %>%
  print(n = 20)
```

```{r}
tibble::tibble(
  Model = c("Quasi-Poisson", "Negative Binomial", "Zero Inflated Poisson", "Zero Inflated Negative Binomial"),
  Significant = c(12, 17, 22,3)
)
```

```{r}
library(ggVennDiagram)
venn_data <- list(
    QuasiPoisson = genus_qpoisson$response_variable,
    NegativeBinomial = genus_nb2$response_variable,
    ZeroInflatedPoisson = genus_zpoisson$response_variable,
    ZeroInflatedNegativeBinomial = genus_znegbin$response_variable
    )
ggVennDiagram(venn_data, label = "count") + 
  scale_fill_gradient(low = "white", high = "dodgerblue")
```

#### Species level
- Quasipoisson regression model
```{r}
results_species_qpoisson <- 
  map_df(names(species_tbl_t)[2:102], function(response) {
    model <- glm(as.formula(paste("`", response, "`", "~ Type", sep = "")), 
                 data = species_tbl_t, 
                 family = quasipoisson)
    tidy(model, conf.int = TRUE) %>%
      # Add a column for the response variable
      mutate(response_variable = response)
    }) %>% 
  filter(term == "TypeControl") %>% 
  arrange(p.value) 

species_qpoi <- results_species_qpoisson %>% 
  filter(p.value < 0.05/102) %>%
  dplyr::select(response_variable, p.value) %>%
  print(n = 20)
```

- Negative binomial regression model
```{r}
results_species_nb2 <- 
  map_df(names(species_tbl_t)[2:102], function(response) {
    # print(response)
    model <- glm(as.formula(paste("`", response, "`", "~ Type", sep = "")), 
                 family = negative.binomial(20),
                 data = species_tbl_t)
    tidy(model, conf.int = TRUE) %>%
      # Add a column for the response variable
      mutate(response_variable = response)
    }) %>% 
  filter(term == "TypeControl") %>% 
  arrange(p.value)

species_nb2 <- results_species_nb2 %>% 
  filter(p.value < 0.05/102) %>%
  dplyr::select(response_variable, p.value) %>%
  print(n = 20)
```
- Zero inflated poisson
```{r}
results_species_zpoi <- 
  map_df(names(species_tbl_t)[2:102], function(response) {
    # print(response)
    species_tbl_t[[response]] <- round(species_tbl_t[[response]])
    model <- zeroinfl(as.formula(paste("`", response, "`", "~ Type", sep = "")), 
                      data = species_tbl_t, dist = "poisson")
    coefs <- summary(model)$coefficients$count
    coefs1 <- summary(model)$coefficients$zero
    p_values <- data.frame(
      term = rownames(coefs),
      p.value = coefs[2, "Pr(>|z|)"],
      p.value_zero = coefs1[2, "Pr(>|z|)"],
      response_variable = response
    )
    
    p_values
  }) %>% 
  filter(term == "TypeControl") %>% 
  arrange(p.value)

species_zpoi <- results_species_zpoi %>% 
  filter(p.value < 0.05/102) %>%
  filter(p.value_zero < 0.05/102) %>%
  dplyr::select(response_variable, p.value, p.value_zero) %>%
  print(n = 20)
```

- Zero inflated negative binomial
```{r}
results_species_znegbin <- 
  map_df(names(species_tbl_t)[2:102], function(response) {
    # print(response)
    species_tbl_t[[response]] <- round(species_tbl_t[[response]])
    model <- zeroinfl(as.formula(paste("`", response, "`", "~ Type", sep = "")), 
                      data = species_tbl_t, dist = "negbin")
    coefs <- summary(model)$coefficients$count
    coefs1 <- summary(model)$coefficients$zero
    p_values <- data.frame(
      term = rownames(coefs),
      p.value = coefs[2, "Pr(>|z|)"],
      p.value_zero = coefs1[2, "Pr(>|z|)"],
      response_variable = response
    )
    
    p_values
  }) %>% 
  filter(term == "TypeControl") %>% 
  arrange(p.value)

species_znegbin <- results_species_znegbin %>% 
  filter(p.value < 0.05/102) %>%
  filter(p.value_zero < 0.05/102) %>%
  dplyr::select(response_variable, p.value, p.value_zero) %>%
  print(n = 20)
```

```{r}
tibble::tibble(
  Model = c("Quasi-Poisson", "Negative Binomial", "Zero Inflated Poisson", "Zero Inflated Negative Binomial"),
  Significant = c(20, 27, 49, 8)
)
```

  + Create a Venn Diagram to summarize the significant microbiome clusters overlaps from these models.

```{r}
venn_data <- list(
  QuasiPoisson = species_qpoi$response_variable,
  NegativeBinomial = species_nb2$response_variable,
  ZeroInflatedPoisson = species_zpoi$response_variable,
  ZeroInflatedNegativeBinomial = species_znegbin$response_variable
)

ggVennDiagram(venn_data, label = "count") + 
  scale_fill_gradient(low = "white", high = "dodgerblue")
```
  

### Question 2
- Evaluate empirical type I error rate of the methods adopted in this study for 41 genus and 101 species.
  + Example code provided below.
  + Summarize your median (IQR) of the type I error across genus and species in a table.
  
#### Genus level
- Quasipoisson regression model
```{r}
# Set the number of columns 
num_columns <- 100
num_rows <- dim(genus_tbl_t)[1] 
set.seed(10)

# Generate the tibble with 100 columns of random normal values
type1e <- function(genusname) {
  sim_tbl <- tibble(
    y = genus_tbl_t %>% dplyr::select(all_of(genusname)) %>% pull ,
    as_tibble(
    matrix(sample(0:1, num_columns * num_rows, replace=T, prob=c(p_control, p_case)), 
           nrow = num_rows, ncol = num_columns, 
         dimnames = list(NULL, paste0("x", 1:num_columns)))))

  results <- map_df(names(sim_tbl)[-1], 
                  ~ tidy(glm(reformulate(.x, response = "y"), 
                             data = sim_tbl,                              
                             family = quasipoisson)), 
                  .id = "variable") %>%
              filter(term != "(Intercept)")
  return(mean(results$p.value < 0.05))
}

type1e_tbl = tibble(genus_name = genusnames, 
       etype_1_error = map_dbl(genusnames, type1e)) 
median(type1e_tbl$etype_1_error)
IQR(type1e_tbl$etype_1_error)
```

- Negative binomial regression model
```{r}
# Set the number of columns 
num_columns <- 100
num_rows <- dim(genus_tbl_t)[1] 
set.seed(10)

# Generate the tibble with 100 columns of random normal values
type1e <- function(genusname) {
  sim_tbl <- tibble(
    y = genus_tbl_t %>% dplyr::select(all_of(genusname)) %>% pull,
    as_tibble(
    matrix(sample(0:1, num_columns * num_rows, replace=T, prob=c(p_control, p_case)), 
           nrow = num_rows, ncol = num_columns, 
         dimnames = list(NULL, paste0("x", 1:num_columns)))))

  results <- map_df(names(sim_tbl)[-1], 
                  ~ tidy(glm(reformulate(.x, response = "y"), 
                             data = sim_tbl, 
                             family = negative.binomial(20),
                             control = glm.control(maxit = 100))), 
                  .id = "variable") %>%
              filter(term != "(Intercept)")
  return(mean(results$p.value < 0.05))
}

type1e_tbl = tibble(genus_name = genusnames, 
       etype_1_error = map_dbl(genusnames, type1e)) 
median(type1e_tbl$etype_1_error)
IQR(type1e_tbl$etype_1_error)
```

- Zero inflated poisson
```{r}
# Set the number of columns 
num_columns <- 100
num_rows <- dim(genus_tbl_t)[1] 
set.seed(10)

# Generate the tibble with 100 columns of random normal values
type1e <- function(genusname) {
  sim_tbl <- tibble(
    y = genus_tbl_t %>% dplyr::select(all_of(genusname)) %>% pull %>% round,
    as_tibble(
    matrix(sample(0:1, num_columns * num_rows, replace=T, prob=c(p_control, p_case)), 
           nrow = num_rows, ncol = num_columns, 
         dimnames = list(NULL, paste0("x", 1:num_columns)))))

  extract_pvalues <- function(variable, data) {
    model <- zeroinfl(reformulate(variable, response = "y"), data = sim_tbl, 
                      dist = "poisson")
    summary_model <- summary(model)
    coefs <- summary_model$coefficients$count
    p_values <- data.frame(
      term = rownames(coefs),
      p.value = coefs[, "Pr(>|z|)"],
      variable = variable
    )
    p_values
  }
  results <- map_df(names(sim_tbl)[-1], ~ extract_pvalues(.x, sim_tbl))
  
  filtered_results <- results %>%
    filter(term != "(Intercept)")
  
  return(mean(filtered_results$p.value < 0.05))
}


type1e_tbl = tibble(genus_name = genusnames, 
       etype_1_error = map_dbl(genusnames, type1e)) 
median(type1e_tbl$etype_1_error)
IQR(type1e_tbl$etype_1_error)
```

- Zero inflated negative binomial
```{r}
# Set the number of columns 
num_columns <- 100
num_rows <- dim(genus_tbl_t)[1] 
set.seed(10)

# Generate the tibble with 100 columns of random normal values
type1e <- function(genusname) {
  sim_tbl <- tibble(
    y = genus_tbl_t %>% dplyr::select(all_of(genusname)) %>% pull %>% round,
    as_tibble(
    matrix(sample(0:1, num_columns * num_rows, replace=T, prob=c(p_control, p_case)), 
           nrow = num_rows, ncol = num_columns, 
         dimnames = list(NULL, paste0("x", 1:num_columns)))))

  extract_pvalues <- function(variable, data) {
    model <- zeroinfl(reformulate(variable, response = "y"), data = sim_tbl, 
                      dist = "negbin")
    summary_model <- summary(model)
    coefs <- summary_model$coefficients$count
    p_values <- data.frame(
      term = rownames(coefs),
      p.value = coefs[, "Pr(>|z|)"],
      variable = variable
    )
    p_values
  }
  results <- map_df(names(sim_tbl)[-1], ~ extract_pvalues(.x, sim_tbl))
  
  filtered_results <- results %>%
    filter(term != "(Intercept)")
  
  return(mean(filtered_results$p.value < 0.05))
}


type1e_tbl = tibble(genus_name = genusnames, 
       etype_1_error = map_dbl(genusnames, type1e)) 
median(type1e_tbl$etype_1_error)
IQR(type1e_tbl$etype_1_error)
```



#### Species level
- Quasipoisson regression model
```{r}
# Set the number of columns 
num_columns <- 100
num_rows <- 992 
set.seed(10)

# Generate the tibble with 100 columns of random normal values
type1e <- function(speciesnames) {
  sim_tbl <- tibble(
    y = species_tbl_t %>% dplyr::select(all_of(speciesnames)) %>% pull,
    as_tibble(
    matrix(sample(0:1, num_columns * num_rows, replace=T, prob=c(p_control, p_case)), 
          nrow = num_rows, ncol = num_columns, 
         dimnames = list(NULL, paste0("x", 1:num_columns)))))

  results <- map_df(names(sim_tbl)[-1], 
                  ~ tidy(glm(reformulate(.x, response = "y"), 
                             data = sim_tbl), family = quasipoisson), 
                  .id = "variable") %>%
              filter(term != "(Intercept)")
  return(mean(results$p.value < 0.05))
}

type1e_tbl = tibble(species_name = names(species_tbl_t)[2:102], 
       etype_1_error = map_dbl(names(species_tbl_t)[2:102], type1e))
median(type1e_tbl$etype_1_error)
IQR(type1e_tbl$etype_1_error)
```

- Negative binomial regression model
```{r}
# Set the number of columns
num_columns <- 100
num_rows <- 992


# Generate the tibble with 100 columns of random normal values
type1e <- function(speciesname) {
  sim_tbl <- tibble(
    y = species_tbl_t %>% 
      dplyr::select(all_of(speciesname)) %>% pull, 
    as_tibble(
    matrix(sample(0:1, num_columns * num_rows, replace=T, prob=c(p_control, p_case)),
           nrow = num_rows, ncol = num_columns, 
           dimnames = list(NULL, paste0("x", 1:num_columns)))))

    results <- map_df(names(sim_tbl)[-1], 
                 ~ tidy(glm(reformulate(.x, response = "y"), 
                             data = sim_tbl, 
                             family = negative.binomial(30),
                             control = glm.control(maxit = 100))), 
                  .id = "variable") %>%
              filter(term != "(Intercept)")
  return(mean(results$p.value < 0.05))
}

type1e_tbl = tibble(species_name = names(species_tbl_t)[2:102], 
       etype_1_error = map_dbl(names(species_tbl_t)[2:102], type1e))
median(type1e_tbl$etype_1_error)
IQR(type1e_tbl$etype_1_error)
```

- Zero inflated poisson regression model
```{r}
# Set the number of columns
num_columns <- 100
num_rows <- 992


# Generate the tibble with 100 columns of random normal values
type1e <- function(speciesname) {
  sim_tbl <- tibble(
    y = species_tbl_t %>% 
      dplyr::select(all_of(speciesname)) %>% pull %>% round, 
    as_tibble(
    matrix(sample(0:1, num_columns * num_rows, replace=T, prob=c(p_control, p_case)),
           nrow = num_rows, ncol = num_columns, 
           dimnames = list(NULL, paste0("x", 1:num_columns)))))

    extract_pvalues <- function(variable, data) {
    model <- zeroinfl(reformulate(variable, response = "y"), data = sim_tbl, 
                      dist = "poisson")
    summary_model <- summary(model)
    coefs <- summary_model$coefficients$count
    p_values <- data.frame(
      term = rownames(coefs),
      p.value = coefs[, "Pr(>|z|)"],
      variable = variable
    )
    p_values
  }
  results <- map_df(names(sim_tbl)[-1], ~ extract_pvalues(.x, sim_tbl))
  
  filtered_results <- results %>%
    filter(term != "(Intercept)")
  
  return(mean(filtered_results$p.value < 0.05))
}

type1e_tbl = tibble(species_name = names(species_tbl_t)[2:102], 
       etype_1_error = map_dbl(names(species_tbl_t)[2:102], type1e))
median(type1e_tbl$etype_1_error, na.rm = TRUE)
IQR(type1e_tbl$etype_1_error, na.rm = TRUE)
```

- Zero inflated negative binomial regression model
```{r}
# Set the number of columns
num_columns <- 100
num_rows <- 992


# Generate the tibble with 100 columns of random normal values
type1e <- function(speciesname) {
  sim_tbl <- tibble(
    y = species_tbl_t %>% 
      dplyr::select(all_of(speciesname)) %>% pull %>% round, 
    as_tibble(
    matrix(sample(0:1, num_columns * num_rows, replace=T, prob=c(p_control, p_case)),
           nrow = num_rows, ncol = num_columns, 
           dimnames = list(NULL, paste0("x", 1:num_columns)))))

    extract_pvalues <- function(variable, data) {
    model <- zeroinfl(reformulate(variable, response = "y"), data = sim_tbl, 
                      dist = "poisson")
    summary_model <- summary(model)
    coefs <- summary_model$coefficients$count
    p_values <- data.frame(
      term = rownames(coefs),
      p.value = coefs[, "Pr(>|z|)"],
      variable = variable
    )
    p_values
  }
  results <- map_df(names(sim_tbl)[-1], ~ extract_pvalues(.x, sim_tbl))
  
  filtered_results <- results %>%
    filter(term != "(Intercept)")
  
  return(mean(filtered_results$p.value < 0.05))
}

type1e_tbl = tibble(species_name = names(species_tbl_t)[2:102], 
       etype_1_error = map_dbl(names(species_tbl_t)[2:102], type1e))
median(type1e_tbl$etype_1_error)
IQR(type1e_tbl$etype_1_error)
```

```{r}
tibble::tibble(
  Method = c("Quasi-Poisson", "Negative Binomial", "Zero Inflated Poisson", "Zero Inflated Negative Binomial"),
  Genus = c("0.05 IQR(0.03)" , "0.07 IQR(0.05)", "0.97 IQR(0.05)", "0 IQR()"),
  Species = c("0.03 IQR(0.04)", "0.07 IQR(0.05)", "0.97 IQR(0.0525)", "0 IQR()")
)
```





### Question 3 
- Comment on the model fit (Question 1) of different models using, for example, AIC, etc., the control type I error rate, and Summarize results from the best model.





### Evaluate Type I Error rate
- This section evaluates the Type I error rate of the models by simulating data under the null hypothesis of no association between the microbiome and the outcome variable.
- We simulate binary labels 100 times using a Bernoulli distribution with a probability of 0.512 (estimated from the data) for each sample.
- We evaluate the association between each genus and the outcome variable using different regression models, document the proportion of significant associations using threshold of 0.05 over 100 replicates (e.g., empirical type I error).
- We repeat the same process for the species level data.
- If the empirical type I error rate is close to the nominal type I error rate (0.05), then the model is considered to have good control over the Type I error rate.
- If the empirical type I error rate is much lower than the nominal type I error rate, then the model is considered to be conservative.
- If the empirical type I error rate is much higher than the nominal type I error rate, then the model is considered to be inflated (i.e., false positive rate is higher than expected).





# Re-do midterm questions (40 pts)
## Question 2 and 8
### Q2.1
Deviance = $2*\log\frac{L_{\Omega}}{L_{\omega}}$, where $L_{\Omega}$ is the likelihood of the saturated model and $L_{\omega}$ is the likelihood of the fitted model. Deviance values from R output is 0.95443 and we know AIC = Deviance + 2p, where p is the number of parameter so $37.666 = -2*l_{\omega} +2 \times 4$. So the log-likelihood = -14.833.

### Q2.2
$H_{0}$: The fitted model has a good fit (no big difference from saturated model) vs $H_{1}$: The fitted model is lack of fit.
The deviance is 0.95443 and degree of freedom is 4. The 0.05 quantile of chi-square distribution with 4 degree of freedom is 9.488. Since 0.95443 < 9.488, we fail to reject the null hypothesis and conclude that the fitted model has a good fit.

### Q2.3
Difference between deviance of null model and fitted model is 21.33202 - 0.95443 = 20.37759. The difference between degree of freedom of null model and fitted model is 7-4 = 3. The 0.05 quantile of chi-square distribution with 3 degree of freedom is 7.815. Since 20.37759 > 7.815, we reject the null hypothesis and conclude that the fitted model is better than the null model.

### Q2.4
log(odds) $= -2.6163 + 0.6223 + 0.6157 +0.362 = -1.0163$. The odds ratio is $e^{-1.0163} = 0.362$. 

### Q2.5
We will have 8 parameters (1 intercept, 3 main effects, 3 2-way interactions, 1 3-way interactions). Since the number of parameter of this new fitted model is equal to the saturated model, the deviance of this new fitted model will be 0.

### Q2.6
The Hessian matrix of a logistic model is: $H_{\beta} = \sum_{i} \left[-\frac{e^{-\mathbf{x}_{i}^{T} \boldsymbol{\beta}}}{(1 + e^{-\mathbf{x}_{i}^{T} \boldsymbol{\beta}})^{2}} \mathbf{x}_{i}\mathbf{x}_{i}^{T} \right]$.  Let $\mathbf{v}$ be any vector such that $\mathbf{v} \in \mathbb{R}^{q}$ and $\mathbf{v} \ne \mathbf{0}$. So we can get
$$
\mathbf{v}^{T} (\mathbf{-H_{\beta}}) \mathbf{v} = \mathbf{v}^{T} \left( \sum_{i=1}^n \frac{e^{-\mathbf{x}_{i}^T \boldsymbol{\beta}}}{(1 + e^{-\mathbf{x}_{i}^{T} \boldsymbol{\beta}})^{2}} \mathbf{x}_{i}\mathbf{x}_{i}^{T} \right) \mathbf{v} = \sum_{i=1}^n \frac{e^{-\mathbf{x}_{i}^T \boldsymbol{\beta}}}{(1 + e^{-\mathbf{x}_{i}^{T} \boldsymbol{\beta}})^{2}} (\mathbf{x}_{i}^{T} \mathbf{v})^2 \ge 0,
$$
since $\frac{e^{-\mathbf{x}_{i}^T \boldsymbol{\beta}}}{(1 + e^{-\mathbf{x}_{i}^{T} \boldsymbol{\beta}})^{2}} \ge 0$ and $(\mathbf{x}_{i}^{T} \mathbf{v})^2 \ge 0$. So the negative Hessian is a positive semidefinite matrix and then the log-likelihood function of logistic regression is a concave function. If we use probit, it is still concave.

### Q2.7
It will be the same as our fitted model because binomial model can be viewed as the weighted bernoulli model.

### Q8.1
There are 7 parameters. Four for predictors and three for intercepts.

### Q8.2
$\log\left[\frac{P(D \le 1|x)}{P(D > 1|x)}\right] = \theta_{1} -age*\beta_{age}-gender*\beta_{gender}-smoke*\beta_{smoke}-hpt*\beta_{hpt}$

So $\log\left[\frac{P(D \ge 2|x)}{P(D < 2|x)}\right] = -\theta_{1} +age*\beta_{age}+gender*\beta_{gender}+smoke*\beta_{smoke}+hpt*\beta_{hpt}$

So log(odds) = $\log\left[\frac{P(D \ge 2|x)}{P(D < 2|x)}\right]=40\beta_{age}+\beta_{gender}+\beta_{smoke}-\theta_{1}$. 

So the odds = $e^{(40\beta_{age}+\beta_{gender}+\beta_{smoke}-\theta_{1})}$

### Q8.3
$\log\left[\frac{P(D \le 0|x_{1})}{P(D > 0|x_{1})}\right] - \log\left[\frac{P(D \le 0|x_{2})}{P(D > 0|x_{2})}\right] = -\beta_{smoke}-\beta_{hpt}$

So $\log\left[\frac{P(D \ge 1|x_{1})}{P(D < 1|x_{1})}\right] - \log\left[\frac{P(D \ge 1|x_{2})}{P(D < 1|x_{2})}\right] = \beta_{smoke}+\beta_{hpt}$

So odds ratio = $e^{(\beta_{smoke}+\beta_{hpt})}$

### Q8.4
$\log\left[\frac{P(D \ge 3|x_{1})}{P(D < 3|x_{1})}\right] - \log\left[\frac{P(D \ge 3|x_{2})}{P(D < 3|x_{2})}\right] = \beta_{smoke}+\beta_{hpt}$

So odds ratio = $e^{(\beta_{smoke}+\beta_{hpt})}$

So odds (D $\ge$ 3) = $e^{(age*\beta_{age}+gender*\beta_{gender}+smoke*\beta_{smoke}+hpt*\beta_{hpt}-\theta_{2})}$

### Q8.5
odds ratio = $e^{(-\beta_{smoke}-\beta_{hpt})}$


## Optional: Question 4, 6, 9
### Q4.1
Because matching was done on age, race and county of residence. Age here is used to determine the matched group of each individual. It is a confounding factor.

### Q4.2
$e^{coef} = e^{0.66698} = 1.948$. So the odds ratio is 1.948.

### Q4.3
Because conditional logistic regression is used to analyze matched case-control studies, which is a type of survival analysis. This type of regression is well-suited for situations where the data involve matched pairs or strata, such as in survival analysis

### Q4.4
Conditional logistic regression should be preferred over unconditional logistic regression in situations where data are collected in matched pairs or sets, and where the matching is intended to control for confounding variables.

### Q6.1
5 parameters in one category and there are total 3 categories, so there are 15 parameters in total.

### Q6.2
odds (severe vs. none) = $e^{(\beta_{2,0} + 40*\beta_{2,age} + \beta_{2,gender} + \beta_{2, hpt})}$

### Q6.3
odds ratio for male vs. female, comparing mild disease to none = $e^{\beta_{1,gender}}$

### Q6.4
The small model does not have `smoke` and `hpt`. So $H_{0}$: $\beta_{g,smoke} = \beta_{g, hpt} = 0$ for g = 1, 2, 3 vs $H_{1}$: at least one of them is not 0. The test statistic = Deviance of small model - Deviance of large model. The distribution of this test statistic is chi-square distribution with degree of freedom = 6.

### Q6.5
In each category, there will be 2 more parameters. So in total, there will be 6 parameters added to the model.

### Q9
logit link function
$$
\eta = g(p) = \log \frac{p}{1-p}
$$
Latent variable distribution is logistic distribution.

Probit link function
$$
\eta = g(p) = \Phi^{-1}(p),
$$
where $\Phi$ is the cumulative distribution function (cdf) of a standard normal. Latent variable distribution is normal distribution.

Cauchit link function
$$
\eta = g(p) = \tan((p - 1/2) \pi).
$$
Latent variable distribution is Cauchy distribution.






