---
title: "Biostat 200C Homework 3"
subtitle: Due May 10 @ 11:59PM
author: "Ziheng Zhang_606300061"
date: today
format:
  html:
    theme: cosmo
    embed-resources: true
    number-sections: true
    toc: true
    toc-depth: 4
    toc-location: left
    code-fold: false
engine: knitr
knitr:
  opts_chunk: 
    fig.align: 'center'
    # fig.width: 6
    # fig.height: 4
    message: FALSE
    cache: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = FALSE)
```

To submit homework, please upload both Rmd and html files to Bruinlearn by the deadline.

## Q1. Concavity of Poisson regression log-likelihood 

Let $Y_1,\ldots,Y_n$ be independent random variables with $Y_i \sim \text{Poisson}(\mu_i)$ and $\log \mu_i = \mathbf{x}_i^T \boldsymbol{\beta}$, $i = 1,\ldots,n$.

### Q1.1

Write down the log-likelihood function.

**Answer:**
We have 
$$
\mathbb{P}(Y_i = y_i) = e^{-\mu_i} \frac{\mu_i^{y_i}}{y_i!}.
$$
So the log-likelihood is
$$
\begin{eqnarray*}
\ell(\boldsymbol{\beta}) &=& \sum_{i=1}^n y_i \log \mu_i - \mu_i - \log y_i! \\
&=& \sum_{i=1}^n \left[ y_i \mathbf{x}_i^T \boldsymbol{\beta} - e^{\mathbf{x}_i^T \boldsymbol{\beta}} - \log y_i! \right]
\end{eqnarray*}
$$

### Q1.2

Derive the gradient vector and Hessian matrix of the log-likelhood function with respect to the regression coefficients $\boldsymbol{\beta}$. 

**Answer:**
The gradient is
$$
\begin{eqnarray*}
\ell(\boldsymbol{\beta}) &=& \sum_{i=1}^n \left[ y_i \mathbf{x}_i^T \boldsymbol{\beta} - e^{\mathbf{x}_i^T \boldsymbol{\beta}} - \log y_i! \right] \\
\nabla_{\beta}(\mathbf{x}_{i}^{T} \boldsymbol{\beta}) &=& \nabla_{\beta}(1 \beta_0 + \mathbf{x}_{i1} \beta_1 + \cdots + \mathbf{x}_{iq} \beta_q) &=& \begin{pmatrix} 1 \\ \mathbf{x}_{i1} \\ \vdots \\ \mathbf{x}_{iq} \end{pmatrix} &=& \mathbf{x}_{i} \\
\nabla_{\beta} \ell(\boldsymbol{\beta}) &=& \sum_{i=1}^n \left[y_{i}\nabla_{\beta} (\mathbf{x}_{i}^{T} \boldsymbol{\beta}) - \nabla_{\beta} (e^{\mathbf{x}_i^T \boldsymbol{\beta}}) \right] \\
&=& \sum_{i=1}^n \left[y_{i} \mathbf{x}_{i} - e^{\mathbf{x}_{i}^T \boldsymbol{\beta}} \nabla_{\beta} \mathbf{x}_{i}^{T} \boldsymbol{\beta} \right] \\
&=& \sum_{i=1}^n \left[y_{i} \mathbf{x}_{i} - e^{\mathbf{x}_{i}^T \boldsymbol{\beta}} \mathbf{x}_{i} \right]
\end{eqnarray*}
$$

The Hessian matrix is
$$
\begin{eqnarray*}
H_{\beta} &=& \nabla^{2}_{\beta} \ell(\boldsymbol{\beta})\\
&=& \sum_{i=1}^n \left[-\nabla_{\beta}(e^{\mathbf{x}_{i}^T \boldsymbol{\beta}} \mathbf{x}_{i})\right] \\
&=& \sum_{i=1}^n \left[-\nabla_{\beta}(e^{\mathbf{x}_{i}^T \boldsymbol{\beta}}) \begin{pmatrix} 1 \\ \mathbf{x}_{i1} \\ \vdots \\ \mathbf{x}_{iq} \end{pmatrix}\right] \\
&=& \sum_{i=1}^n \left[-e^{\mathbf{x}_{i}^{T} \boldsymbol{\beta}} \mathbf{x}_{i} \mathbf{x}_{i}^{T} \right]
\end{eqnarray*}
$$

### Q1.3

Show that the log-likelihood function of the log-linear model is a concave function in regression coefficients $\boldsymbol{\beta}$. (Hint: show that the negative Hessian is a positive semidefinite matrix.)

**Answer:**
The negative Hessian matrix is as follows:
$$
\begin{eqnarray*}
-H_{\beta} = \sum_{i=1}^n \left[e^{\mathbf{x}_{i}^{T} \boldsymbol{\beta}} \mathbf{x}_{i} \mathbf{x}_{i}^{T} \right]
\end{eqnarray*}
$$
Let $\mathbf{v}$ be any vector such that $\mathbf{v} \in \mathbb{R}^{q}$ and $\mathbf{v} \ne \mathbf{0}$. So we can get
$$
\mathbf{v}^{T} (\mathbf{-H_{\beta}}) \mathbf{v} = \mathbf{v}^{T} \left( \sum_{i=1}^n e^{\mathbf{x}_{i}^{T} \boldsymbol{\beta}} \mathbf{x}_{i} \mathbf{x}_{i}^{T}  \right) \mathbf{v} = \sum_{i=1}^n e^{\mathbf{x}_{i}^{T} \boldsymbol{\beta}} (\mathbf{x}_{i}^{T} \mathbf{v})^2 \ge 0,
$$
since $e^{\mathbf{x}_{i}^{T} \boldsymbol{\beta}} \ge 0$ and $(\mathbf{x}_{i}^{T} \mathbf{v})^2 \ge 0$.

So the negative Hessian is a positive semidefinite matrix and then the log-likelihood function of the log-linear model is a concave function. 

### Q1.4

Show that for the fitted values $\widehat{\mu}_i$ from maximum likelihood estimates
$$
\sum_i \widehat{\mu}_i = \sum_i y_i.
$$
Therefore the deviance reduces to
$$
D = 2 \sum_i y_i \log \frac{y_i}{\widehat{\mu}_i}.
$$

## Q2. Show negative binomial distribution mean and variance 

Recall the probability mass function of negative binomial distribution is 
$$
\mathbb{P}(Y = y) = \binom{y + r - 1}{r - 1} (1 - p)^r p^y, \quad y = 0, 1, \ldots
$$
Show $\mathbb{E}Y = \mu = rp / (1 - p)$ and $\operatorname{Var} Y = r p / (1 - p)^2$.

## Q3. ELMR Chapter 5 Exercise 5 (page 100)

## Q4. Uniform association 

For the uniform association when all two-way interactions are included, i.e., 
$$
\log \mathbb{E}Y_{ijk} = \log p_{ijk} = \log n + \log p_i + \log p_j + \log p_k + \log p_{ij} + \log p_{ik} + \log p_{jk}.
$$

Proof the odds ratio (or log of odds ratio) across all stratum $k$ 
$$
\log \frac{\mathbb{E}Y_{11k}\mathbb{E}Y_{22k}}{\mathbb{E}Y_{12k}\mathbb{E}Y_{21k}}
$$

is a constant, i.e., the estimated effect of the interaction term "i:j" in the uniform association model 
