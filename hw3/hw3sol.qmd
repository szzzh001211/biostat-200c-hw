---
title: "Biostat 200C Homework 3"
subtitle: Due May 10 @ 11:59PM
author: "Ziheng Zhang_606300061"
date: today
format:
  html:
    theme: cosmo
    embed-resources: true
    number-sections: true
    toc: true
    toc-depth: 4
    toc-location: left
    code-fold: false
engine: knitr
knitr:
  opts_chunk: 
    fig.align: 'center'
    # fig.width: 6
    # fig.height: 4
    message: FALSE
    cache: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = FALSE)
```

To submit homework, please upload both Rmd and html files to Bruinlearn by the deadline.

## Q1. Concavity of Poisson regression log-likelihood 

Let $Y_1,\ldots,Y_n$ be independent random variables with $Y_i \sim \text{Poisson}(\mu_i)$ and $\log \mu_i = \mathbf{x}_i^T \boldsymbol{\beta}$, $i = 1,\ldots,n$.

### Q1.1

Write down the log-likelihood function.

**Answer:**
We have 
$$
\mathbb{P}(Y_i = y_i) = e^{-\mu_i} \frac{\mu_i^{y_i}}{y_i!}.
$$
So the log-likelihood is
$$
\begin{eqnarray*}
\ell(\boldsymbol{\beta}) &=& \sum_{i=1}^n y_i \log \mu_i - \mu_i - \log y_i! \\
&=& \sum_{i=1}^n \left[ y_i \mathbf{x}_i^T \boldsymbol{\beta} - e^{\mathbf{x}_i^T \boldsymbol{\beta}} - \log y_i! \right]
\end{eqnarray*}
$$

### Q1.2

Derive the gradient vector and Hessian matrix of the log-likelhood function with respect to the regression coefficients $\boldsymbol{\beta}$. 

**Answer:**
The gradient is
$$
\begin{eqnarray*}
\ell(\boldsymbol{\beta}) &=& \sum_{i=1}^n \left[ y_i \mathbf{x}_i^T \boldsymbol{\beta} - e^{\mathbf{x}_i^T \boldsymbol{\beta}} - \log y_i! \right] \\
\nabla_{\beta}(\mathbf{x}_{i}^{T} \boldsymbol{\beta}) &=& \nabla_{\beta}(1 \beta_0 + \mathbf{x}_{i1} \beta_1 + \cdots + \mathbf{x}_{iq} \beta_q) &=& \begin{pmatrix} 1 \\ \mathbf{x}_{i1} \\ \vdots \\ \mathbf{x}_{iq} \end{pmatrix} &=& \mathbf{x}_{i} \\
\nabla_{\beta} \ell(\boldsymbol{\beta}) &=& \sum_{i=1}^n \left[y_{i}\nabla_{\beta} (\mathbf{x}_{i}^{T} \boldsymbol{\beta}) - \nabla_{\beta} (e^{\mathbf{x}_i^T \boldsymbol{\beta}}) \right] \\
&=& \sum_{i=1}^n \left[y_{i} \mathbf{x}_{i} - e^{\mathbf{x}_{i}^T \boldsymbol{\beta}} \nabla_{\beta} \mathbf{x}_{i}^{T} \boldsymbol{\beta} \right] \\
&=& \sum_{i=1}^n \left[y_{i} \mathbf{x}_{i} - e^{\mathbf{x}_{i}^T \boldsymbol{\beta}} \mathbf{x}_{i} \right]
\end{eqnarray*}
$$

The Hessian matrix is
$$
\begin{eqnarray*}
H_{\beta} &=& \nabla^{2}_{\beta} \ell(\boldsymbol{\beta})\\
&=& \sum_{i=1}^n \left[-\nabla_{\beta}(e^{\mathbf{x}_{i}^T \boldsymbol{\beta}} \mathbf{x}_{i})\right] \\
&=& \sum_{i=1}^n \left[-\nabla_{\beta}(e^{\mathbf{x}_{i}^T \boldsymbol{\beta}}) \begin{pmatrix} 1 \\ \mathbf{x}_{i1} \\ \vdots \\ \mathbf{x}_{iq} \end{pmatrix}\right] \\
&=& \sum_{i=1}^n \left[-e^{\mathbf{x}_{i}^{T} \boldsymbol{\beta}} \mathbf{x}_{i} \mathbf{x}_{i}^{T} \right]
\end{eqnarray*}
$$

### Q1.3

Show that the log-likelihood function of the log-linear model is a concave function in regression coefficients $\boldsymbol{\beta}$. (Hint: show that the negative Hessian is a positive semidefinite matrix.)

**Answer:**
The negative Hessian matrix is as follows:
$$
\begin{eqnarray*}
-H_{\beta} = \sum_{i=1}^n \left[e^{\mathbf{x}_{i}^{T} \boldsymbol{\beta}} \mathbf{x}_{i} \mathbf{x}_{i}^{T} \right]
\end{eqnarray*}
$$
Let $\mathbf{v}$ be any vector such that $\mathbf{v} \in \mathbb{R}^{q}$ and $\mathbf{v} \ne \mathbf{0}$. So we can get
$$
\mathbf{v}^{T} (\mathbf{-H_{\beta}}) \mathbf{v} = \mathbf{v}^{T} \left( \sum_{i=1}^n e^{\mathbf{x}_{i}^{T} \boldsymbol{\beta}} \mathbf{x}_{i} \mathbf{x}_{i}^{T}  \right) \mathbf{v} = \sum_{i=1}^n e^{\mathbf{x}_{i}^{T} \boldsymbol{\beta}} (\mathbf{x}_{i}^{T} \mathbf{v})^2 \ge 0,
$$
since $e^{\mathbf{x}_{i}^{T} \boldsymbol{\beta}} \ge 0$ and $(\mathbf{x}_{i}^{T} \mathbf{v})^2 \ge 0$.

So the negative Hessian is a positive semidefinite matrix and then the log-likelihood function of the log-linear model is a concave function. 

### Q1.4

Show that for the fitted values $\widehat{\mu}_i$ from maximum likelihood estimates
$$
\sum_i \widehat{\mu}_i = \sum_i y_i.
$$
Therefore the deviance reduces to
$$
D = 2 \sum_i y_i \log \frac{y_i}{\widehat{\mu}_i}.
$$
**Answer:**
The deviance for the Poisson regression is
$$
\begin{eqnarray*}
  D &=& 2 \sum_i [y_i \log(y_i) - y_i] - 2 \sum_i [y_i \log (\widehat{\mu}_i) - \widehat{\mu}_i] \\
  &=& 2 \sum_i [y_i \log \frac{y_i}{\widehat{\mu}_i} - (y_i - \widehat{\mu}_i)], 
\end{eqnarray*}
$$
where $\widehat{\mu}_i$ are the fitted values from the model.

In order to find maximum likelihood estimates, we need to maximize the log-likelihood function. The log-likelihood function is
$$
\ell(\boldsymbol{\beta}) = \sum_i \left[ y_i \mathbf{x}_i^T \boldsymbol{\beta} - e^{\mathbf{x}_i^T \boldsymbol{\beta}} - \log y_i! \right]
$$
Taking the derivative with respect to $\boldsymbol{\beta}$, we get
$$
\nabla_{\beta} \ell(\boldsymbol{\beta}) = \sum_i \left[y_{i} \mathbf{x}_{i} - e^{\mathbf{x}_{i}^T \boldsymbol{\beta}} \mathbf{x}_{i} \right] = 0
$$
Since $\mathbf{x}_{i} = \begin{pmatrix} 1 \\ \mathbf{x}_{i1} \\ \vdots \\ \mathbf{x}_{iq} \end{pmatrix}$, then if we consider the first row of $\mathbf{x}_{i}$ (the first element in $\mathbf{x}_{i}$ is 1), we can get
$$
\sum_i y_{i}  = \sum_ie^{\mathbf{x}_{i}^T \boldsymbol{\hat{\beta}}}
$$
Since the fitted values $\widehat{\mu}_i$ are from maximum likelihood estimates, then 
$$
\begin{eqnarray*}
\sum_i y_{i}  = \sum_ie^{\mathbf{x}_{i}^T \boldsymbol{\hat{\beta}}} &=& \sum_i \widehat{\mu}_i \\
\sum_i (y_{i} - \widehat{\mu}_i) &=& 0
\end{eqnarray*}
$$
So the deviance reduces to
$$
D = 2 \sum_i y_i \log \frac{y_i}{\widehat{\mu}_i}.
$$

## Q2. Show negative binomial distribution mean and variance 

Recall the probability mass function of negative binomial distribution is 
$$
\mathbb{P}(Y = y) = \binom{y + r - 1}{r - 1} (1 - p)^r p^y, \quad y = 0, 1, \ldots
$$
Show $\mathbb{E}Y = \mu = rp / (1 - p)$ and $\operatorname{Var} Y = r p / (1 - p)^2$.

**Answer:**
$y$ is the number of failures until the $r$th success with probability of failure $p$. 
$$
\begin{aligned}
M_Y(t) &= \mathbb{E}e^{tY} = \sum_{y=0}^\infty e^{ty} \binom{y + r - 1}{r - 1} (1 - p)^r p^y \\
&= \sum_{y=0}^\infty \binom{y + r - 1}{r - 1} \left[1 - pe^t \right]^r \left[pe^t \right]^y \cdot \frac{(1-p)^r}{(1-pe^t)^r}  \\
&= \left\{\frac{1-p}{1-pe^t}\right\}^r
\end{aligned}
$$
Define $\psi_{Y}(t) = log M_{Y}(t) = \log \left\{\frac{1-p}{1-pe^t}\right\}^r = r\log(1-p) - r\log(1 - pe^{t})$. Then 
$$
\begin{eqnarray*}
\mathbb{E}Y &=& \psi_{Y}'(t)|_{t=0} = \frac{rpe^{t}}{1 - pe^{t}}|_{t=0} = \frac{rp}{1 - p} \\
\operatorname{Var} Y &=& \psi_{Y}''(t)|_{t=0} = \frac{rp^2e^{2t}+rpe^{t}(1-pe^t)}{(1 - pe^{t})^2}|_{t=0} = \frac{rp}{(1 - p)^2}
\end{eqnarray*}
$$
So $\mathbb{E}Y = \mu = \frac{rp}{1 - p}$ and $\operatorname{Var} Y =  \frac{rp}{(1 - p)^2}$.

## Q3. ELMR Chapter 5 Exercise 5 (page 100)
The `dvisits` data comes from the Australian Health Survey of 1977–1978 and consist of 5190 single adults where young and old have been oversampled.

### Q3.1
(a) Make plots which show the relationship between the response variable, `doctorco`, and the potential predictors, `age` and `illness`.
```{r}
library(faraway)
library(tidyverse)
library(ggplot2)

dvisits |>
  head(width = Inf)
```

```{r}
ggplot(dvisits) +
  geom_point(aes(x = age*100, y = doctorco), alpha = 0.5, position = "jitter") +
  labs(x = "Age", y = "Doctorco", title = "Doctorco vs Age")

ggplot(dvisits) +
  geom_point(aes(x = illness, y = doctorco), alpha = 0.5, position = "jitter") +
  labs(x = "Illness", y = "Doctorco", title = "Doctorco vs Illness")
```
**Answer:** For the first plot, we see that the `doctorco` is higher for younger and older people. For the second plot, it is surprising that the `doctorco` is higher for people with number of illnesses less than or equal to 2.


### Q3.2
(b) Combine the predictors `chcond1` and `chcond2` into a single three-level factor. Make an appropriate plot showing the relationship between this factor and the response. Comment.

```{r}
dvisits1 <- dvisits |>
  mutate(chronic = case_when(
    chcond1 == 1 ~ "Not limited",
    chcond2 == 1 ~ "Limited",
    TRUE ~ "No chronic"))
dvisits1$chronic <- factor(dvisits1$chronic, 
                           levels = c("No chronic", "Not limited", "Limited"))

ggplot(dvisits1) +
  geom_point(aes(x = chronic, y = doctorco), alpha = 0.5, position = "jitter") +
  labs(x = "Chronic condition", y = "Doctorco", 
       title = "Doctorco vs Chronic condition")
```
**Answer:** The plot shows that people with chronic conditions but not limited in activity have higher `doctorco` than the other two conditions, especially when the number of `doctorco` is less than 2.5. People with chronic conditions and limited in activity have higher `doctorco` on average.

### Q3.3
(c) Build a Poisson regression model with `doctorco` as the response and `sex`, `age`, `agesq`, `income`, `levyplus`, `freepoor`, `freerepa`, `illness`, `actdays`, `hscore` and the three-level condition factor as possible predictor variables. Considering the deviance of this model, does this model fit the data?

```{r}
library(gtsummary)
mod1 <- glm(doctorco ~ sex + age + agesq + income + levyplus + freepoor + 
              freerepa + illness + actdays + hscore + chronic, 
            family = poisson, data = dvisits1)
summary(mod1)
```
**Answer:** The deviance of the model is 4379.5 with 5177 degrees of freedom. The deviance is less than the degrees of freedom, so the model fits the data well.

### Q3.4
(d) Plot the residuals and the fitted values — why are there lines of observations on the plot? Make a QQ plot of the residuals and comment.
```{r}
dvisits1 |>
  mutate(devres  = residuals(mod1, type = "deviance"), 
         linpred = predict(mod1, type = "link")) %>%
  ggplot + 
  geom_point(mapping = aes(x = linpred, y = devres)) + 
  labs(x = "Linear predictor", y = "Deviance residual")
```
**Answer:** The lines of observations are due to the fact that the `doctorco` is a discrete variable (count variable) and we use a Poisson regression model.

```{r}
qqnorm(residuals(mod1, type = "response"))
```
**Answer:** The QQ plot of the residuals shows that the residuals are not normally distributed. There are several outliers in residuals.

### Q3.5
(e) Use a stepwise AIC-based model selection method. What sort of person would be predicted to visit the doctor the most under your selected model?
```{r}
mod2 <- step(mod1, trace = F)
summary(mod2)
```
**Answer:** According to the coefficients of the AIC selected model, the person who is predicted to visit the doctor the most is a person who is female, older, has a lower income, is covered by private health insurance fund, is poor, has a higher number of illnesses, has a higher number of days of reduced activity in past two weeks due to illness or injury, has higher general health questionnaire score (bad health), and has a chronic condition.

### Q3.6
(f) For the last person in the dataset, compute the predicted probability distribution for their visits to the doctor, i.e., give the probability they visit 0, 1, 2, etc. times.
```{r}
last_person <- dvisits1[nrow(dvisits1), ]
dpois(0:5, lambda = predict(mod2, newdata = last_person, type = "response"))
```

### Q3.7
(g) Tabulate the frequencies of the number of doctor visits. Compute the expected frequencies of doctor visits under your most recent model. Compare the observed with the expected frequencies and comment on whether it is worth fitting a zero-inflated count model.
```{r}
library(pscl)
table(dvisits1$doctorco)

tibble(ocount = table(dvisits1$doctorco)[1:10],    
       pcount = colSums(predprob(mod2)[, 1:10]), 
       count  = 0:9) %>%
  ggplot(mapping = aes(x = pcount, y = ocount, label = count)) + 
  geom_point() + 
  geom_text(nudge_y = 200) +
  labs(x = "Predicted", y = "Observed") +
  geom_abline(intercept = 0, slope = 1)
```
**Answer:** Since the observed and predicted frequencies of the zeroes are close to each other, it is not worth fitting a zero-inflated count model.

### Q3.8
(h) Fit a comparable (Gaussian) linear model and graphically compare the fits. Describe how they differ.
```{r}
mod3 <- lm(doctorco ~ sex + age + income + levyplus + freepoor + illness + 
             actdays + hscore + chronic, data = dvisits1)
pred <- predict(mod2, dvisits1, type = "response")
pred1 <- predict(mod3, dvisits1, type = "response")
ggplot() +
  geom_point(aes(x = pred, y = pred1), alpha = 0.5) +
  labs(x = "Poisson model", y = "Gaussian model") +
  geom_abline(intercept = 0, slope = 1)

summary(mod2)
summary(mod3)
```
**Answer:** From the plot, when the predicted values of the Poisson model are small, the predicted values of the Gaussian model are larger. When the predicted values of the Poisson model are large, the predicted values of the Gaussian model are smaller. The predicted values of the Gaussian model can be negative, which is not possible for the Poisson model. From the summary of the two models, they are different in fits. If we check from the standard errors and p-values, the Poisson model is better than the Gaussian model.

## Q4. Uniform association 

For the uniform association when all two-way interactions are included, i.e., 
$$
\log \mathbb{E}Y_{ijk} = \log p_{ijk} = \log n + \log p_i + \log p_j + \log p_k + \log p_{ij} + \log p_{ik} + \log p_{jk}.
$$

Proof the odds ratio (or log of odds ratio) across all stratum $k$ 
$$
\log \frac{\mathbb{E}Y_{11k}\mathbb{E}Y_{22k}}{\mathbb{E}Y_{12k}\mathbb{E}Y_{21k}}
$$

is a constant, i.e., the estimated effect of the interaction term "i:j" in the uniform association model 

**Answer:**
$$
\begin{eqnarray*}
\log \frac{\mathbb{E}Y_{11k}\mathbb{E}Y_{22k}}{\mathbb{E}Y_{12k}\mathbb{E}Y_{21k}} &=& \log (\mathbb{E}Y_{11k}) + \log (\mathbb{E}Y_{22k}) - \log (\mathbb{E}Y_{12k}) - \log (\mathbb{E}Y_{21k}) \\
\log (\mathbb{E}Y_{11k}) &=& \log p_{11k} = \log n + \log p_1 + \log p_1 + \log p_k + \log p_{11} + \log p_{1k} + \log p_{1k}\\
\log (\mathbb{E}Y_{22k}) &=& \log p_{22k} = \log n + \log p_2 + \log p_2 + \log p_k + \log p_{22} + \log p_{2k} + \log p_{2k}\\
\log (\mathbb{E}Y_{12k}) &=& \log p_{12k} = \log n + \log p_1 + \log p_2 + \log p_k + \log p_{12} + \log p_{1k} + \log p_{2k}\\
\log (\mathbb{E}Y_{21k}) &=& \log p_{21k} = \log n + \log p_2 + \log p_1 + \log p_k + \log p_{21} + \log p_{2k} + \log p_{1k}\\
\end{eqnarray*}
$$

So the odds ratio (or log of odds ratio) across all stratum $k$ is
$$
\log \frac{\mathbb{E}Y_{11k}\mathbb{E}Y_{22k}}{\mathbb{E}Y_{12k}\mathbb{E}Y_{21k}} = \log p_{11} + \log p_{22} - \log p_{12} - \log p_{21} \\
$$
and it is a constant, i.e., the estimated effect of the interaction term "i:j" in the uniform association model.





