---
title: "Biostat 200C Homework 5"
subtitle: Due May 31  @ 11:59PM
author: "Ziheng Zhang_606300061"
date: today
format:
  html:
    theme: cosmo
    embed-resources: true
    number-sections: true
    toc: true
    toc-depth: 4
    toc-location: left
    code-fold: false
engine: knitr
knitr:
  opts_chunk: 
    fig.align: 'center'
    # fig.width: 6
    # fig.height: 4
    message: FALSE
    cache: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = FALSE)
```
## Q1. Balanced one-way ANOVA random effects model

Consider the balanced one-way ANOVA random effects model with $a$ levels and $n$ observations in each level
$$
y_{ij} = \mu + \alpha_i + \epsilon_{ij}, \quad i=1,\ldots,a, \quad j=1,\ldots,n.
$$
where $\alpha_i$ are iid from $N(0,\sigma_\alpha^2)$, $\epsilon_{ij}$ are iid from $N(0, \sigma_\epsilon^2)$. 

1. Derive the ANOVA estimate for $\mu$, $\sigma_\alpha^2$, and $\sigma_{\epsilon}^2$. Specifically show that
\begin{eqnarray*}
  \mathbb{E}(\bar y_{\cdot \cdot}) &=& \mathbb{E} \left( \frac{\sum_{ij} y_{ij}}{na} \right) = \mu \\
  \mathbb{E} (\text{SSE}) &=& \mathbb{E} \left[ \sum_{i=1}^a \sum_{j=1}^n (y_{ij} - \bar{y}_{i \cdot})^2 \right] = a(n-1) \sigma_{\epsilon}^2 \\
  \mathbb{E} (\text{SSA}) &=& \mathbb{E} \left[ \sum_{i=1}^a \sum_{j=1}^n (\bar{y}_{i \cdot} - \bar{y}_{\cdot \cdot})^2 \right] = (a-1)(n \sigma_{\alpha}^2 + \sigma_{\epsilon}^2),
\end{eqnarray*}
which can be solved to obtain ANOVA estimate
\begin{eqnarray*}
\widehat{\mu} &=& \frac{\sum_{ij} y_{ij}}{na}, \\
\widehat{\sigma}_{\epsilon}^2 &=& \frac{\text{SSE}}{a(n-1)}, \\
\widehat{\sigma}_{\alpha}^2 &=& \frac{\text{SSA}/(a-1) - \widehat{\sigma}_{\epsilon}^2}{n}.
\end{eqnarray*}

**Answer:**

a. $\mathbb{E}(\bar y_{\cdot \cdot})$
$$
\begin{eqnarray*}
\mathbb{E}(\bar y_{\cdot \cdot}) &=& \mathbb{E} \left( \frac{\sum_{ij} y_{ij}}{na} \right) = \mathbb{E}\left( \frac{\sum_{ij} \mu + \alpha_i + \epsilon_{ij}}{na}\right) \\
&=& \frac{\mathbb{E}(\sum_{ij}\mu)}{na} + 0 + 0 \\
&=& \mu + 0 + 0 = \mu,
\end{eqnarray*}
$$
since $\alpha_i$ are iid from $N(0, \sigma_{\alpha}^2)$, $\epsilon_{ij}$ are iid $N(0,\sigma_{\epsilon}^2)$, then $\mathbb{E}(\alpha_i) = 0$, $\mathbb{E}(\epsilon_{ij}) = 0$.

b. $\mathbb{E} (\text{SSE})$
$$
\begin{eqnarray*}
\mathbb{E} \left[ \sum_{i=1}^a \sum_{j=1}^n (y_{ij} - \bar{y}_{i \cdot})^2 \right] &=& \sum_{i=1}^a \mathbb{E}\left[ \sum_{j=1}^n (y_{ij} - \bar{y}_{i \cdot})^2 \right] \\
y_{ij} &=& \mu + \alpha_i+\epsilon_{ij} \\
\bar{y}_{i \cdot} &=& \frac{1}{n} \sum_{j=1}^n(\mu + \alpha_i + \epsilon_{ij}) = \mu + \alpha_i + \bar{\epsilon}_{i \cdot} \\
y_{ij} - \bar{y}_{i \cdot} &=& \epsilon_{ij} - \bar{\epsilon}_{i \cdot} \\
\mathbb{E}\left[\sum_{j=1}^n (y_{ij} - \bar{y}_{i \cdot})^2 \right]&=&\mathbb{E}\left[\sum_{j=1}^n(\epsilon_{ij} - \bar{\epsilon}_{i \cdot})^2\right] = \mathbb{E} \left[(n-1)S^2_{\epsilon}\right]=(n-1)\sigma_\epsilon^2,\\ && \text{where } S^2_{\epsilon} \text{ is the sample variance of } \epsilon_{ij} \\

\mathbb{E} (\text{SSE})&=&\mathbb{E} \left[ \sum_{i=1}^a \sum_{j=1}^n (y_{ij} - \bar{y}_{i \cdot})^2 \right] = a(n-1) \sigma_{\epsilon}^2
\end{eqnarray*}
$$
c. $\mathbb{E} (\text{SSA})$
$$
\begin{eqnarray*}
\mathbb{E} (\text{SSA}) &=&\mathbb{E} \left[ \sum_{i=1}^a \sum_{j=1}^n (\bar{y}_{i \cdot} - \bar{y}_{\cdot \cdot})^2 \right]\\
&=&\mathbb{E} \left[ \sum_{i=1}^a \sum_{j=1}^n (\mu + \alpha_i + \bar{\epsilon}_{i \cdot} - \mu + \bar{\alpha} + \bar{\epsilon_{\cdot \cdot}})^2 \right]\\
&=&\mathbb{E} \left[ \sum_{i=1}^a \sum_{j=1}^n (\alpha_i - \bar{\alpha})^2 \right] + \mathbb{E} \left[ \sum_{i=1}^a \sum_{j=1}^n (\bar{\epsilon}_{i \cdot} - \bar{\epsilon_{\cdot \cdot}})^2 \right] \\
&=& n\mathbb{E}\left[\sum_{i=1}^a(\alpha_i - \bar{\alpha})^2\right] + n\mathbb{E}\left[\sum_{i=1}^a(\bar{\epsilon}_{i \cdot} - \bar{\epsilon_{\cdot \cdot}})^2\right] \\
&=& n\mathbb{E}\left[(a-1)S^2_{\alpha}\right] + n\mathbb{E}\left[(a-1)S^2_\bar{\epsilon}\right] \\
&=& n(a-1) \sigma_{\alpha}^2 + n(a-1)\frac{\sigma_{\epsilon}^2}{n}\\
&=& n(a-1) \sigma_{\alpha}^2 + (a-1)\sigma_{\epsilon}^2\\
&=& (a-1)(n\sigma_{\alpha}^2 + \sigma_{\epsilon}^2)
\end{eqnarray*}
$$

2. Derive the MLE estimate for $\mu$, $\sigma_\alpha^2$, and $\sigma_{\epsilon}^2$. Hint: write down the log-likelihood and find the maximizer.

**Answer:**

$$
y_{ij} = \mu + \alpha_i + \epsilon_{ij}, \quad i=1,\ldots,a, \quad j=1,\ldots,n.
$$
where $\alpha_i$ are iid from $N(0,\sigma_\alpha^2)$, $\epsilon_{ij}$ are iid from $N(0, \sigma_\epsilon^2)$.

So $y_{ij}$ are iid from $N(\mu, \sigma_\alpha^2 + \sigma_\epsilon^2)$. So the log-likelihood function is
$$
\begin{eqnarray*}
L(\mu, \sigma_{\alpha}^2, \sigma_{\epsilon}^2) &=& \prod_{i=1}^a \prod_{j=1}^n \frac{1}{\sqrt{2\pi(\sigma_{\alpha}^2 + \sigma_{\epsilon}^2)}}\exp\left(-\frac{(y_{ij} - \mu)^2}{2(\sigma_{\alpha}^2 + \sigma_{\epsilon}^2)}\right) \\
\ell(\mu, \sigma_{\alpha}^2, \sigma_{\epsilon}^2) &=& -\frac{na}{2}\log(2\pi) - \frac{na}{2}\log(\sigma_{\epsilon}^2+\sigma_{\alpha}^2)  - \frac{1}{2(\sigma_{\epsilon}^2+\sigma_{\alpha}^2)}\sum_{i=1}^a\sum_{j=1}^n(y_{ij}-\mu)^2\\
\frac{\partial \ell}{\partial \mu} &=& \frac{1}{\sigma^2_\alpha + \sigma^2_\epsilon} \sum_{i=1}^a \sum_{j=1}^n (y_{ij} - \mu) = 0\\
\hat{\mu} &=& \frac{1}{na}\sum_{i=1}^a \sum_{j=1}^n y_{ij} = \bar{y}_{\cdot \cdot} \tag{1}\\
\frac{\partial \ell}{\partial \sigma^2_\alpha} &=& -\frac{na}{2(\sigma_{\epsilon}^2+\sigma_{\alpha}^2)} + \frac{1}{2(\sigma_{\epsilon}^2+\sigma_{\alpha}^2)^2}\sum_{i=1}^a\sum_{j=1}^n(y_{ij}-\mu)^2 = 0\\
\frac{\partial \ell}{\partial \sigma^2_\epsilon} &=& -\frac{na}{2(\sigma_{\epsilon}^2+\sigma_{\alpha}^2)} + \frac{1}{2(\sigma_{\epsilon}^2+\sigma_{\alpha}^2)^2}\sum_{i=1}^a\sum_{j=1}^n(y_{ij}-\mu)^2 = 0\\
\hat{\sigma}^2_\alpha + \hat{\sigma}^2_\epsilon &=& \frac{1}{na}\sum_{i=1}^a\sum_{j=1}^n(y_{ij}-\mu)^2 = \frac{1}{na}\sum_{i=1}^a\sum_{j=1}^n(y_{ij}-\bar{y}_{\cdot \cdot})^2 \tag{2}
\end{eqnarray*}
$$

3. (**Optional**) Derive the REML estimate for $\mu$, $\sigma_\alpha^2$, and $\sigma_{\epsilon}^2$. 

4. For all three estimates, check that your results match those we obtained using R for the `pulp` example in class.

**Answer:**

For MLE, we have $\hat{\mu} = \bar{y}_{\cdot \cdot}$ and $\hat{\sigma}^2_\alpha + \hat{\sigma}^2_\epsilon = \frac{1}{na}\sum_{i=1}^a\sum_{j=1}^n(y_{ij}-\bar{y}_{\cdot \cdot})^2$. We can use the following R code to check the results.
```{r}
library(lme4)
library(faraway)
smod <- lmer(bright ~ 1 + (1 | operator), data = pulp, REML = FALSE)
summary(smod)
```
In the R code, the estimate of $\mu$ is the intercept, $60.4$, and the estimate of $\sigma^2_\alpha + \sigma^2_\epsilon$ is the sum of variance components, $0.04575 + 0.10625 = 0.152$.

```{r}
mean(pulp$bright)
```
So the MLE estimate of $\mu$ matches the result we obtained in R. 

```{r}
sum((pulp$bright - mean(pulp$bright))^2)/(5*4)
```
So the MLE estimate of $\sigma^2_\alpha + \sigma^2_\epsilon$ matches the result we obtained in R.

For REML, we have . We can use the following R code to check the results.


## Q2. Estimation of random effects

1. Assume the conditional distribution
$$
\mathbf{y} \mid \boldsymbol{\gamma} \sim N(\mathbf{X} \boldsymbol{\beta} + \mathbf{Z} \boldsymbol{\gamma}, \sigma^2 \mathbf{I}_n)
$$
and the prior distribution
$$
\boldsymbol{\gamma} \sim N(\mathbf{0}_q, \boldsymbol{\Sigma}).
$$
Then by the Bayes theorem, the posterior distribution is
\begin{eqnarray*}
f(\boldsymbol{\gamma} \mid \mathbf{y}) &=& \frac{f(\mathbf{y} \mid \boldsymbol{\gamma}) \times f(\boldsymbol{\gamma})}{f(\mathbf{y})}, \end{eqnarray*}
where $f$ denotes corresponding density. Show that the posterior distribution is a multivariate normal with mean
$$
\mathbb{E} (\boldsymbol{\gamma} \mid \mathbf{y}) = \boldsymbol{\Sigma} \mathbf{Z}^T (\mathbf{Z} \boldsymbol{\Sigma} \mathbf{Z}^T + \sigma^2 \mathbf{I})^{-1} (\mathbf{y} - \mathbf{X} \boldsymbol{\beta}).
$$
**Answer:**

By Bayes theorem, we have
$$
\begin{eqnarray*}
f(\boldsymbol{\gamma} \mid \mathbf{y}) &=& \frac{f(\mathbf{y} \mid \boldsymbol{\gamma}) \times f(\boldsymbol{\gamma})}{f(\mathbf{y})}\\
&\propto& \exp\left(-\frac{1}{2\sigma^2}(\mathbf{y} - \mathbf{X} \boldsymbol{\beta} - \mathbf{Z} \boldsymbol{\gamma})^T (\mathbf{y} - \mathbf{X} \boldsymbol{\beta} - \mathbf{Z} \boldsymbol{\gamma})\right) \times \exp\left(-\frac{1}{2} \boldsymbol{\gamma}^T \boldsymbol{\Sigma}^{-1} \boldsymbol{\gamma}\right)\\
&=& \exp\left(-\frac{1}{2\sigma^2}(\mathbf{y} - \mathbf{X} \boldsymbol{\beta} - \mathbf{Z} \boldsymbol{\gamma})^T (\mathbf{y} - \mathbf{X} \boldsymbol{\beta} - \mathbf{Z} \boldsymbol{\gamma}) - \frac{1}{2} \boldsymbol{\gamma}^T \boldsymbol{\Sigma}^{-1} \boldsymbol{\gamma}\right)\\
&\propto& \exp\left(-\frac{1}{2}\left[\boldsymbol{\gamma}^T\left(\frac{\mathbf{Z}^T\mathbf{Z}}{\sigma^2}+\boldsymbol{\Sigma}^{-1}\right)\boldsymbol{\gamma}-\frac{2}{\sigma^2}(\boldsymbol{\gamma}^T\mathbf{Z}^T(\mathbf{y} - \mathbf{X} \boldsymbol{\beta})) \right]\right), \text{since }\boldsymbol{\gamma}^T\mathbf{Z}^T(\mathbf{y} - \mathbf{X} \boldsymbol{\beta}) \text{ is a scalar}\\
&=& \exp\left(-\frac{1}{2}(\boldsymbol{\gamma} - A^{-1}b)^TA(\boldsymbol{\gamma} - A^{-1}b) -b^TA^{-1}b\right), \text{where } A = \left(\frac{\mathbf{Z}^T\mathbf{Z}}{\sigma^2}+\boldsymbol{\Sigma}^{-1}\right)^{-1} \text{ and } b = \frac{1}{\sigma^2}\mathbf{Z}^T(\mathbf{y} - \mathbf{X} \boldsymbol{\beta})\\
&\propto& \exp\left(-\frac{1}{2}(\boldsymbol{\gamma} - A^{-1}b)^TA(\boldsymbol{\gamma} - A^{-1}b)\right)\\
\mathbb{E} (\boldsymbol{\gamma} \mid \mathbf{y}) &=& A^{-1}b = \left(\frac{\mathbf{Z}^T\mathbf{Z}}{\sigma^2}+\boldsymbol{\Sigma}^{-1}\right)^{-1}\frac{1}{\sigma^2}\mathbf{Z}^T(\mathbf{y} - \mathbf{X} \boldsymbol{\beta}) = \boldsymbol{\Sigma} \mathbf{Z}^T (\mathbf{Z} \boldsymbol{\Sigma} \mathbf{Z}^T + \sigma^2 \mathbf{I})^{-1} (\mathbf{y} - \mathbf{X} \boldsymbol{\beta})
\end{eqnarray*}
$$
2. For the balanced one-way ANOVA random effects model, show that the posterior mean of random effects is always a constant (less than 1) multiplying the corresponding fixed effects estimate.


## Q3. ELMR Exercise 11.1 (p251)

The ratdrink data consist of five weekly measurements of body weight for 27 rats. The first 10 rats are on a control treatment while 7 rats have thyroxine added to their drinking water. Ten rats have thiouracil added to their water.

### Q3.1
(a) Plot the data showing how weight increases with age on a single panel, taking care to distinguish the three treatment groups. Now create a three-panel plot, one for each group. Discuss what can be seen.

**Answer:**
```{r}
library(ggplot2)
library(gridExtra)
library(tidyverse)
ggplot(data = ratdrink) +
  stat_summary(mapping = aes(x = weeks, y = wt, group = treat, 
                             color = as.factor(treat)), geom = "line", 
               fun.y = mean) +
  labs(x = "Week", y = "Weight",
       title = "Rat Weight Over Time",
       col = "Treatment")

control <- ratdrink |>
  filter(treat == "control") |>
  ggplot(aes(x = weeks, y = wt)) + 
  geom_point() +
  labs(title = "Control Rat Weight Over Time")

thyroxine <- ratdrink |>
  filter(treat == "thyroxine") |>
  ggplot(aes(x = weeks, y = wt)) + 
  geom_point() +
  labs(title = "Thyroxine Rat Weight Over Time")

thiouracil <- ratdrink |>
  filter(treat == "thiouracil") |>
  ggplot(aes(x = weeks, y = wt)) + 
  geom_point() +
  labs(title = "Thiouracil Rat Weight Over Time")

grid.arrange(control, thyroxine, thiouracil, ncol = 2)
```
Control rats and thyroxine treated rats have similar weight gain over time, while thiouracil treated rats have a lower weight gain over time. From the one-panel plot, the mean value of weight gain for thiouracil also shows a lower weight gain compared to the other two groups.

### Q3.2
(b) Fit a linear longitudinal model that allows for a random slope and intercept for each rat. Each group should have a different mean line. Give interpretation for the following estimates:

i. The fixed effect intercept term.\
ii. The interaction between thiouracil and week.\
iii. The intercept random effect SD.

**Answer:**
```{r}
library(lme4)
m1 <- lmer(wt ~ weeks * treat + (weeks | subject), data = ratdrink)
summary(m1)
```
+ The fixed effect intercept term: The estimated weight of the control group at week 0 is 52.88.

+ The interaction between thiouracil and week: For every one week increase, the estimated weight gain for the thiouracil group is -9.37 weights lower than that for the control group.

+ The intercept random effect SD: The standard deviation between subjects within the same treatment group is 5.7.

### Q3.3
(c) Check whether there is a significant treatment effect.

**Answer:**
```{r}
library(pbkrtest)
m2 <- lmer(wt ~ weeks + (weeks | subject), data = ratdrink)
KRmodcomp(m1, m2)
```
The p-value is 0.0001215, which is less than 0.05. Therefore, there is a significant treatment effect.

### Q3.4
(d) Construct diagnostic plots showing the residuals against the fitted values and a QQ plot of the residuals. Interpret.

**Answer:**
```{r}
plot(resid(m1) ~ fitted(m1), xlab = "Fitted", ylab = "Residuals")
abline(h=0)
```
The residuals are randomly scattered around the 0 line and there is no special pattern, indicating that the linearity and constant variance assumptions are met.

```{r}
qqnorm(resid(m1), main = "")
```
The residuals are close to a straight line, indicating that the normality assumption is met.

### Q3.5
(e) Construct confidence intervals for the parameters of the model. Which random effect terms may not be significant? Is the thyroxine group significantly different from the control group?

**Answer:**
```{r}
confint(m1, method = "boot")
```
`sig01` is the variance of the random intercepts, `sig02` is the covariance between the random intercepts and slopes, and `sig03` is the variance of the random slopes. Both random intercepts and slopes are significant because their intervals do not include 0. The thyroxine group is not significantly different from the control group because the confidence interval includes 0.

## Q4. ELMR Exercise 13.1 (p295)
The ohio data concern 536 children from Steubenville, Ohio and were taken as part of a study on the effects of air pollution. Children were in the study for 4 years from ages 7 to 10. The response was whether they wheezed or not. The variables are:

`resp` an indicator of wheeze status (1 = yes, 0 = no)

`id` an identifier for the child

`age` 7 yrs = −2, 8 yrs = −1, 9 yrs =0, 10 yrs = 1

`smoke` an indicator of maternal smoking at the first year of the study (1=smoker, 0 = nonsmoker)

### Q4.1
(a) Do any of the mothers in the study change their smoking status during the period of observation?

### Q4.2
(b) Construct a table that shows proportion of children who wheeze for 0, 1, 2, 3 or 4 years broken down by maternal smoking status.

### Q4.3
(c) Make plot which shows how the proportion of children wheezing changes by age with a separate line for smoking and nonsmoking mothers.

### Q4.4
(d) Group the data by child to count the total (out of four) years of wheezing. Fit a binomial GLM to this response to check for a maternal smoking effect. Does this prove there is a smoking effect or could there be another plausible explanation?

### Q4.5
(e) Fit a model for each individual response using a GLMM fit using penalized quasi-likelihood. Describe the effects of age and maternal smoking. How do the odds of wheezing change numerically over time?

### Q4.6
(f) Now fit the same model but using adaptive Gaussian-Hermit quadrature. Compare to the previous model fit.

### Q4.7
(g) Use INLA to fit the same model. What does this model say about the effect of age and maternal smoking?

### Q4.8
(h) Use STAN to fit the same model. Check the MCMC diagnostics and again discuss the age and maternal smoking effects.

### Q4.9
(i) Fit the model using GEE. Use an autoregressive rather than exchangeable error structure. Compare the results to the previous model fits. In your model, what indicates that a child who already wheezes is likely to continue to wheeze?

### Q4.10
(j) What is your overall conclusion regarding the effect of age and maternal smoking? Can we trust the GLM result or are the GLMM models preferable?


