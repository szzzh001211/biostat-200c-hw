---
title: "Biostat 200C Homework 5"
subtitle: Due May 31  @ 11:59PM
author: "Ziheng Zhang_606300061"
date: today
format:
  html:
    theme: cosmo
    embed-resources: true
    number-sections: true
    toc: true
    toc-depth: 4
    toc-location: left
    code-fold: false
engine: knitr
knitr:
  opts_chunk: 
    fig.align: 'center'
    # fig.width: 6
    # fig.height: 4
    message: FALSE
    cache: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = FALSE)
```
## Q1. Balanced one-way ANOVA random effects model

Consider the balanced one-way ANOVA random effects model with $a$ levels and $n$ observations in each level
$$
y_{ij} = \mu + \alpha_i + \epsilon_{ij}, \quad i=1,\ldots,a, \quad j=1,\ldots,n.
$$
where $\alpha_i$ are iid from $N(0,\sigma_\alpha^2)$, $\epsilon_{ij}$ are iid from $N(0, \sigma_\epsilon^2)$. 

1. Derive the ANOVA estimate for $\mu$, $\sigma_\alpha^2$, and $\sigma_{\epsilon}^2$. Specifically show that
\begin{eqnarray*}
  \mathbb{E}(\bar y_{\cdot \cdot}) &=& \mathbb{E} \left( \frac{\sum_{ij} y_{ij}}{na} \right) = \mu \\
  \mathbb{E} (\text{SSE}) &=& \mathbb{E} \left[ \sum_{i=1}^a \sum_{j=1}^n (y_{ij} - \bar{y}_{i \cdot})^2 \right] = a(n-1) \sigma_{\epsilon}^2 \\
  \mathbb{E} (\text{SSA}) &=& \mathbb{E} \left[ \sum_{i=1}^a \sum_{j=1}^n (\bar{y}_{i \cdot} - \bar{y}_{\cdot \cdot})^2 \right] = (a-1)(n \sigma_{\alpha}^2 + \sigma_{\epsilon}^2),
\end{eqnarray*}
which can be solved to obtain ANOVA estimate
\begin{eqnarray*}
\widehat{\mu} &=& \frac{\sum_{ij} y_{ij}}{na}, \\
\widehat{\sigma}_{\epsilon}^2 &=& \frac{\text{SSE}}{a(n-1)}, \\
\widehat{\sigma}_{\alpha}^2 &=& \frac{\text{SSA}/(a-1) - \widehat{\sigma}_{\epsilon}^2}{n}.
\end{eqnarray*}

**Answer:**

a. $\mathbb{E}(\bar y_{\cdot \cdot})$
$$
\begin{eqnarray*}
\mathbb{E}(\bar y_{\cdot \cdot}) &=& \mathbb{E} \left( \frac{\sum_{ij} y_{ij}}{na} \right) = \mathbb{E}\left( \frac{\sum_{ij} \mu + \alpha_i + \epsilon_{ij}}{na}\right) \\
&=& \frac{\mathbb{E}(\sum_{ij}\mu)}{na} + 0 + 0 \\
&=& \mu + 0 + 0 = \mu,
\end{eqnarray*}
$$
since $\alpha_i$ are iid from $N(0, \sigma_{\alpha}^2)$, $\epsilon_{ij}$ are iid $N(0,\sigma_{\epsilon}^2)$, then $\mathbb{E}(\alpha_i) = 0$, $\mathbb{E}(\epsilon_{ij}) = 0$.

b. $\mathbb{E} (\text{SSE})$ and  $\mathbb{E} (\text{SSA})$
$$
\text{SSE} = \sum_i \sum_j (y_{ij} - \bar y_{i\cdot})^2 = \mathbf{y}^T \mathbf{A}_1 \mathbf{y}
$$
where 
$$
\mathbf{A}_1 = \begin{pmatrix}
\mathbf{I}_n - n^{-1} \mathbf{1}_n \mathbf{1}_n^T & & \\
& \ddots & \\
& & \mathbf{I}_n - n^{-1} \mathbf{1}_n \mathbf{1}_n^T
\end{pmatrix}.
$$
So
$$
\begin{eqnarray*}
\mathbb{E} (\text{SSE}) &=& \mathbb{E} \mathbf{y}^T \mathbf{A}_1 \mathbf{y} \\
&=& \mathbb{E} \operatorname{tr} \mathbf{A}_1 \mathbf{y} \mathbf{y}^T \\
&=& \operatorname{tr} \mathbf{A}_1 (\sigma_\alpha^2 \mathbf{Z} \mathbf{Z}^T + \sigma_\epsilon^2 \mathbf{I}_{na}) +  \mu^2 \operatorname{tr} \mathbf{A}_1 \mathbf{1}_{na} \mathbf{1}_{na}^T \\
&=& 0 + a (n - 1) \sigma_{\epsilon}^2 + 0 \\
&=& a (n - 1) \sigma_{\epsilon}^2.
\end{eqnarray*}
$$
Now 
$$
\text{SST} = \mathbf{y}^T \mathbf{A}_0 \mathbf{y},
$$
where $\mathbf{A}_0 = \mathbf{I}_{na} - (na)^{-1} \mathbf{1}_{na} \mathbf{1}_{na}^T$. So
$$
\begin{eqnarray*}
\mathbb{E}(\text{SST}) &=& \operatorname{tr} \mathbf{A}_0 (\sigma_\alpha^2 \mathbf{Z} \mathbf{Z}^T + \sigma_\epsilon^2 \mathbf{I}_{na}) + \mu^2 \operatorname{tr} \mathbf{A}_0 \mathbf{1}_{na} \mathbf{1}_{na}^T \\
&=& n (a - 1) \sigma_\alpha^2 + (na - 1) \sigma_{\epsilon}^2 + 0 \\
&=& n (a - 1) \sigma_\alpha^2 + (na - 1) \sigma_{\epsilon}^2.
\end{eqnarray*}
$$
Therefore
$$
\mathbb{E}(\text{SSA}) = \mathbb{E}(\text{SST}) - \mathbb{E}(\text{SSE}) = (a - 1)(n \sigma_{\alpha}^2 + \sigma_{\epsilon}^2).
$$

2. Derive the MLE estimate for $\mu$, $\sigma_\alpha^2$, and $\sigma_{\epsilon}^2$. Hint: write down the log-likelihood and find the maximizer.

**Answer:**

$$
y_{ij} = \mu + \alpha_i + \epsilon_{ij}, \quad i=1,\ldots,a, \quad j=1,\ldots,n.
$$
where $\alpha_i$ are iid from $N(0,\sigma_\alpha^2)$, $\epsilon_{ij}$ are iid from $N(0, \sigma_\epsilon^2)$.

**Answer:**
$$
\begin{eqnarray*}
L(\mu, \sigma^2_\alpha, \sigma^2_\epsilon) &=& \prod_{i=1}^{a} \left[ \frac{1}{(2\pi)^{n/2}|\Omega|^{1/2}} \exp \left\{ -\frac{1}{2} (\mathbf{y}_i - \mu \mathbf{1}_n)^T \Omega^{-1} (\mathbf{y}_i - \mu \mathbf{1}_n) \right\} \right] \\
\\
\text{where } \Omega &=& 
\begin{bmatrix}
\sigma^2_\epsilon + \sigma^2_\alpha & \sigma^2_\alpha & \cdots & \sigma^2_\alpha \\
\sigma^2_\alpha & \sigma^2_\epsilon + \sigma^2_\alpha & \cdots & \sigma^2_\alpha \\
\vdots & \vdots & \ddots & \vdots \\
\sigma^2_\alpha & \sigma^2_\alpha & \cdots & \sigma^2_\epsilon + \sigma^2_\alpha
\end{bmatrix}_{n \times n}
\\
\\
\ell(\mu, \sigma^2_\alpha, \sigma^2_\epsilon) &=& \sum_{i=1}^{a} \left[ -\frac{n}{2}\log(2\pi) - \frac{1}{2} \log|\Omega| - \frac{1}{2} (\mathbf{y}_i - \mu \mathbf{1}_n)^T \Omega^{-1} (\mathbf{y}_i - \mu \mathbf{1}_n) \right] \\
&=&\sum_{i=1}^{a} \left[ -\frac{n}{2}\log(2\pi) - \frac{1}{2} \log|\sigma_{\alpha}^2 \mathbf{1}_n \mathbf{1}_n^T + \sigma_{\epsilon}^2 \mathbf{I}_n| - \frac{1}{2} (\mathbf{y}_i - \mu \mathbf{1}_n)^T (\sigma_{\alpha}^2 \mathbf{1}_n \mathbf{1}_n^T + \sigma_{\epsilon}^2 \mathbf{I}_n)^{-1} (\mathbf{y}_i - \mu \mathbf{1}_n) \right]
\\
\frac{\partial \ell}{\partial \mu} &=& \sum_{i=1}^{a} \Omega^{-1} (\mathbf{y}_i - \mu \mathbf{1}_n)^T \mathbf{1}_n = 0 \\
\Rightarrow \hat{\mu} &=& \frac{\sum_{ij} y_{ij}}{na} = \bar{y}_{\cdot \cdot}\\
\\
\\
\text{By Woodbury formula, }\\
(\sigma_{\alpha}^2 \mathbf{1}_n \mathbf{1}_n^T + \sigma_{\epsilon}^2 \mathbf{I}_n)^{-1} &=& \sigma_{\epsilon}^{-2} \mathbf{I}_{n} - \frac{\sigma_{\epsilon}^{-2} \sigma_{\alpha}^2}{\sigma_{\epsilon}^2 + n\sigma_{\alpha}^2} \mathbf{1}_n \mathbf{1}_n^T \\
\det (\sigma_{\alpha}^2 \mathbf{1}_n \mathbf{1}_n^T + \sigma_{\epsilon}^2 \mathbf{I}_n) &=& \sigma_{\epsilon}^{2n} (1 + n \frac{\sigma_{\alpha}^2}{\sigma_{\epsilon}^2})\\
\\
\ell(\mu, \sigma^2_\alpha, \sigma^2_\epsilon)&=& \sum_{i=1}^{a} \left[ -\frac{n}{2}\log(2\pi) - \frac{1}{2} \log(\sigma_{\epsilon}^{2n} (1 + n \frac{\sigma_{\alpha}^2}{\sigma_{\epsilon}^2})) - \frac{1}{2} (\mathbf{y}_i - \mu \mathbf{1}_n)^T (\sigma_{\epsilon}^{-2} \mathbf{I}_{n} - \frac{\sigma_{\epsilon}^{-2} \sigma_{\alpha}^2}{\sigma_{\epsilon}^2 + n\sigma_{\alpha}^2} \mathbf{1}_n \mathbf{1}_n^T) (\mathbf{y}_i - \mu \mathbf{1}_n) \right]\\
&=&  -\frac{na}{2}\log(2\pi) -\frac{na}{2} \log \sigma_{\epsilon}^2 - \frac{a}{2} \log (1 + n\frac{\sigma_{\alpha}^2}{\sigma_{\epsilon}^2}) - \frac{1}{2\sigma_{\epsilon}^{2}} \frac{\text{SST}(\mu) + n\frac{\sigma_{\alpha}^2}{\sigma_{\epsilon}^2} \text{SSA}}{1 + n \lambda}\\
\end{eqnarray*}
$$
Setting derivative with respect to $\sigma_{\epsilon}^2$ to 0 yields equation
$$
\sigma_{\epsilon}^2 = \frac{\text{SST} - \frac{n\lambda}{1 + n\lambda} \text{SSA}}{na} = \frac{\text{SST} + n \lambda \text{SSE}}{na(1 + n\lambda)}.
$$
Substitution of the above expression into the log-likelihood shows we need to maximize
$$
\begin{eqnarray*}
& & - \frac{na}{2} \log \left( \text{SST} - \frac{n\lambda}{1 + n\lambda} \text{SSA} \right) - \frac{a}{2} \log (1 + n\lambda) \\
&=& - \frac{na}{2} \log \left( \text{SST} + n \lambda \text{SSE} \right) + \frac{(n-1)a}{2} \log (1 + n \lambda).
\end{eqnarray*}
$$
Setting derivative to 0 gives the maximizer
$$
\hat \lambda = \frac{n-1}{n} \frac{\text{SST}}{\text{SSE}} - 1.
$$
Thus
$$
\hat \sigma_{\epsilon}^2 = \frac{\text{SST} - \frac{n \hat \lambda}{1 + n \hat \lambda} \text{SSA}}{na} = \frac{\text{SSE}}{(n-1)a}
$$
(same as ANOVA estimate) and
$$
\hat \sigma_{\alpha}^2 = \frac{\text{SSA}}{an} - \frac{\text{SSE}}{an(n-1)}.
$$

3. (**Optional**) Derive the REML estimate for $\mu$, $\sigma_\alpha^2$, and $\sigma_{\epsilon}^2$. 

Let $K \in \mathbb{R}^{n \times (n-1)}$ (as fixed effects only contain intercept) be a basis of ${\cal N}(X^T)$. Then 
$$
K^T Y \sim N(0, K^T \Omega K).
$$
and the log-likelihood is
$$
\, - \frac 12 \log \det K^T \Omega K - \frac 12 y^T K(K^T \Omega K)^{-1} K^T y.
$$
Setting the derivative with respect to $\sigma_\alpha^2$ and $\sigma_\epsilon^2$ to 0 yields the estimation equations
\begin{eqnarray*}
  \frac{\partial}{\partial \sigma_{\epsilon}^2} \ell &=& - \frac 12 \operatorname{tr} [K(K^T \Omega K)^{-1} K^T] - \frac 12 y^t K(K^T \Omega K)^{-1} K^T K(K^T \Omega K)^{-1} K^T y = 0, \\
  \frac{\partial}{\partial \sigma_{\alpha}^2} \ell &=& - \frac 12 \operatorname{tr} [K(K^T \Omega K)^{-1} K^T Z Z^T] - \frac 12 y^t K(K^T \Omega K)^{-1} K^T Z Z^T K(K^T \Omega K)^{-1} K^T y = 0.
\end{eqnarray*}

We need to show:
$$
K(K^T \Omega K)^{-1} K^T = \Omega^{-1} - \Omega^{-1} X (X^T \Omega^{-1} X)^{-1} X^T \Omega^{-1}.
$$
This is because (1) the two sides
$$
\Omega^{1/2} K(K^T \Omega K)^{-1} K^T \Omega^{1/2} = I - \Omega^{-1/2} X (X^T \Omega^{-1} X)^{-1} X^T \Omega^{-1/2}
$$
are the (unique) orthogonal projection to the space ${\cal C}(X)^\perp = {\cal N}(X^T)$. Or we can prove the relationship as the follows:

Both $KK^{+} := K(K^T K)^{-1}K^T$ and $XX^{+}:=X(X^TX)^{-1}X^T$ are symmetric and idempotent, and $K^TX=0$. Therefore $KK^{+} X = 0$ and $XX^{+} K = 0$. Hence $T = I-KK^{+}-XX^{+}$ is symmetric and idempotent. We have
\begin{eqnarray*}
&\mbox{tr}(TT^T) &= \mbox{tr}(T^2) = \mbox{tr}(T)  \\
& &=N-r_x-r_k = N-r_x-(N-r_x)=0.
\end{eqnarray*}
But $T$ is real, so that $\mbox{tr}(TT^T)=0$ implies that $T=0$. Therefore $KK^{+}=I-XX^{+}$. Because $\Omega=(\Omega^{1/2})^2$. Then, since $(\Omega^{1/2}K)^T\Omega^{-1/2}X=0$, because $K^TX=0$, we can replace $K$ and $X$ by $\Omega^{1/2}K$ and $\Omega^{-1/2}X$ respectively. 

Rewrite $KK^{+}=I-XX^{+}$ as
$$
I-X(X^TX)^{-1}X^T = K(K^TK)^{-1}K^T.
$$
and replace relevant terms by $\Omega^{1/2}K$ and $\Omega^{-1/2}X$, we get
$$
 I-\Omega^{-1/2}X(X^T\Omega^{-1}X)^{-1}X^T\Omega^{-1/2} = \Omega^{1/2}K(K^T\Omega K)^{-1}K^T\Omega^{1/2}.
$$
i.e.,
$$
\Omega^{-1} - \Omega^{-1} X (X^T \Omega^{-1} X)^{-1} X^T \Omega^{-1} = K(K^T \Omega K)^{-1} K^T.
$$
Let's simplify
\begin{eqnarray*}
\mathbf{A} &:=& \Omega^{-1} - \Omega^{-1} X (X^T \Omega^{-1} X)^{-1} X^T \Omega^{-1} \\
&=& \Omega^{-1} - \frac{(1 + n \lambda) \sigma_{\epsilon}^2}{na} \Omega^{-1} X X^T \Omega^{-1} \\
&=& \Omega^{-1} - \frac{(1 + n \lambda) \sigma_{\epsilon}^2}{na} \left( \frac{\sigma_{\epsilon}^{-2}}{1 + n \lambda} \right)^2 \mathbf{1}_{na} \mathbf{1}_{na}^T \\
&=& \Omega^{-1} - \frac{\sigma_{\epsilon}^{-2}}{na(1 + n \lambda)} \mathbf{1}_{na} \mathbf{1}_{na}^T \\
&=& \sigma_\epsilon^{-2} \mathbf{I}_{na} - \frac{\sigma_\epsilon^{-2} \lambda}{1 + n\lambda} \mathbf{Z} \mathbf{Z}^T - \frac{\sigma_{\epsilon}^{-2}}{na(1 + n \lambda)} \mathbf{1}_{na} \mathbf{1}_{na}^T, \\
\mathbf{A}^2 &=& \sigma_{\epsilon}^{-4} \left( \mathbf{I}_{na} - \frac{\lambda}{1 + n\lambda} \mathbf{Z} \mathbf{Z}^T - \frac{1}{na(1 + n \lambda)} \mathbf{1}_{na} \mathbf{1}_{na}^T \right) \left( \mathbf{I}_{na} - \frac{\lambda}{1 + n\lambda} \mathbf{Z} \mathbf{Z}^T - \frac{1}{na(1 + n \lambda)} \mathbf{1}_{na} \mathbf{1}_{na}^T \right) \\
&=& \sigma_{\epsilon}^{-4} \left( \mathbf{I}_{na} + \frac{n\lambda^2}{(1 + n \lambda)^2} \mathbf{Z} \mathbf{Z}^T + \frac{1}{na(1 + n\lambda)^2} \mathbf{1}_{na} \mathbf{1}_{na}^T - \frac{2\lambda}{1 + n\lambda} \mathbf{Z} \mathbf{Z}^T - \frac{2}{na(1+n\lambda)} \mathbf{1}_{na} \mathbf{1}_{na}^T + \frac{2\lambda}{a(1+n\lambda)^2} \mathbf{1}_{na} \mathbf{1}_{na}^T \right) \\
&=& \sigma_{\epsilon}^{-4} \left( \mathbf{I}_{na} - \frac{2\lambda + n\lambda^2}{(1 + n\lambda)^2} \mathbf{Z} \mathbf{Z}^T - \frac{1}{na(1 + n\lambda)^2} \mathbf{1}_{na} \mathbf{1}_{na}^T \right), \\
\mathbf{A} \mathbf{Z} \mathbf{Z}^T \mathbf{A} &=& 
\end{eqnarray*}
The first estimation equations becomes
\begin{eqnarray*}
  \frac{\partial}{\partial \sigma_{\epsilon}^2} \ell &=& - \frac 12 \operatorname{tr} \mathbf{A} - \frac 12 \mathbf{y}^T \mathbf{A} \mathbf{A} \mathbf{y} \\
  &=& - \frac{na}{2} \sigma_{\epsilon}^{-2} + \frac{na\lambda}{2(1 + n \lambda)} \sigma_{\epsilon}^{-2} + \frac{1}{2(1 + n\lambda)} \sigma_{\epsilon}^{-2} \\
  &=& 0.
\end{eqnarray*}

4. For all three estimates, check that your results match those we obtained using R for the `pulp` example in class.

**Answer:**

In class, we have
```{r}
library(faraway)
aovmod <- aov(bright ~ operator, data = pulp) |>
  summary()
```

```{r}
(aovmod[1][[1]][[3]][1] - aovmod[1][[1]][[3]][2]) / 5
aovmod[1][[1]][[3]][2]
```

We can use the following R code to check the results.
```{r}
library(lme4)
smod <- lmer(bright ~ 1 + (1 | operator), data = pulp, REML = FALSE)
summary(smod)
```
In the R code, the estimate of $\mu$ is the intercept, $60.4$, the estimate of $\sigma^2_\alpha = 0.04575$ and $\sigma^2_\epsilon = 0.10625$.

For MLE, we have $\hat{\mu} = \bar{y}_{\cdot \cdot}$, $\hat \sigma_{\epsilon}^2=\frac{\text{SSE}}{(n-1)a}$ and $\hat \sigma_{\alpha}^2 = \frac{\text{SSA}}{an} - \frac{\text{SSE}}{an(n-1)}$. 
```{r}
mean(pulp$bright)
```
So the MLE estimate of $\mu$ matches the result we obtained in R. 

```{r}
aov(bright ~ operator, data = pulp) |>
  summary()
```

```{r}
1.34/(4*5)-1.70/(4*5*4)
1.70/((5-1)*4)
```
So the MLE estimate of $\sigma^2_\alpha$ and $\sigma^2_\epsilon$ matches the result we obtained in R.

## Q2. Estimation of random effects

1. Assume the conditional distribution
$$
\mathbf{y} \mid \boldsymbol{\gamma} \sim N(\mathbf{X} \boldsymbol{\beta} + \mathbf{Z} \boldsymbol{\gamma}, \sigma^2 \mathbf{I}_n)
$$
and the prior distribution
$$
\boldsymbol{\gamma} \sim N(\mathbf{0}_q, \boldsymbol{\Sigma}).
$$
Then by the Bayes theorem, the posterior distribution is
\begin{eqnarray*}
f(\boldsymbol{\gamma} \mid \mathbf{y}) &=& \frac{f(\mathbf{y} \mid \boldsymbol{\gamma}) \times f(\boldsymbol{\gamma})}{f(\mathbf{y})}, \end{eqnarray*}
where $f$ denotes corresponding density. Show that the posterior distribution is a multivariate normal with mean
$$
\mathbb{E} (\boldsymbol{\gamma} \mid \mathbf{y}) = \boldsymbol{\Sigma} \mathbf{Z}^T (\mathbf{Z} \boldsymbol{\Sigma} \mathbf{Z}^T + \sigma^2 \mathbf{I})^{-1} (\mathbf{y} - \mathbf{X} \boldsymbol{\beta}).
$$
**Answer:**

By Bayes theorem, we have
$$
\begin{eqnarray*}
f(\boldsymbol{\gamma} \mid \mathbf{y}) &=& \frac{f(\mathbf{y} \mid \boldsymbol{\gamma}) \times f(\boldsymbol{\gamma})}{f(\mathbf{y})}\\
&\propto& \exp\left(-\frac{1}{2\sigma^2}(\mathbf{y} - \mathbf{X} \boldsymbol{\beta} - \mathbf{Z} \boldsymbol{\gamma})^T (\mathbf{y} - \mathbf{X} \boldsymbol{\beta} - \mathbf{Z} \boldsymbol{\gamma})\right) \times \exp\left(-\frac{1}{2} \boldsymbol{\gamma}^T \boldsymbol{\Sigma}^{-1} \boldsymbol{\gamma}\right)\\
&=& \exp\left(-\frac{1}{2\sigma^2}(\mathbf{y} - \mathbf{X} \boldsymbol{\beta} - \mathbf{Z} \boldsymbol{\gamma})^T (\mathbf{y} - \mathbf{X} \boldsymbol{\beta} - \mathbf{Z} \boldsymbol{\gamma}) - \frac{1}{2} \boldsymbol{\gamma}^T \boldsymbol{\Sigma}^{-1} \boldsymbol{\gamma}\right)\\
&\propto& \exp\left(-\frac{1}{2}\left[\boldsymbol{\gamma}^T\left(\frac{\mathbf{Z}^T\mathbf{Z}}{\sigma^2}+\boldsymbol{\Sigma}^{-1}\right)\boldsymbol{\gamma}-\frac{2}{\sigma^2}(\boldsymbol{\gamma}^T\mathbf{Z}^T(\mathbf{y} - \mathbf{X} \boldsymbol{\beta})) \right]\right), \text{since }\boldsymbol{\gamma}^T\mathbf{Z}^T(\mathbf{y} - \mathbf{X} \boldsymbol{\beta}) \text{ is a scalar}\\
&=&\exp\left(-\frac{1}{2}\left[\boldsymbol{\gamma}^T\left(\sigma^{-2}\mathbf{Z}^T\mathbf{Z}+\boldsymbol{\Sigma}^{-1}\right)\boldsymbol{\gamma}-\frac{2}{\sigma^2}(\boldsymbol{\gamma}^T(\sigma^{-2} \mathbf{Z}^T \mathbf{Z} + \boldsymbol{\Sigma}^{-1}) (\sigma^{-2} \mathbf{Z}^T \mathbf{Z} + \boldsymbol{\Sigma}^{-1})^{-1 }\mathbf{Z}^T(\mathbf{y} - \mathbf{X} \boldsymbol{\beta})) \right]\right)
\end{eqnarray*}
$$
It's clear the covariance of posterior normal distribution is 
$$
(\sigma^{-2} \mathbf{Z}^T \mathbf{Z} + \boldsymbol{\Sigma}^{-1})^{-1} = \boldsymbol{\Sigma} - \boldsymbol{\Sigma} \mathbf{Z}^T (\sigma^2 \mathbf{I_n} + \mathbf{Z} \mathbf{Z}^T)^{-1} \mathbf{Z} \boldsymbol{\Sigma}.
$$
Now by binomial inversion 
$$
\begin{eqnarray*}
  & & \sigma^{-2} (\sigma^{-2} \mathbf{Z}^T \mathbf{Z} + \Sigma^{-1})^{-1 } \mathbf{Z}^T (\mathbf{y} - \mathbf{X} \boldsymbol{\beta}) \\
  &=& \sigma^{-2} [\Sigma \mathbf{Z}^T - \Sigma \mathbf{Z}^T (\mathbf{Z} \Sigma \mathbf{Z}^T + \sigma^2 \mathbf{I})^{-1} \mathbf{Z} \Sigma \mathbf{Z}^T] (\mathbf{y} - \mathbf{X} \boldsymbol{\beta}) \\
  &=& \sigma^{-2} \Sigma \mathbf{Z}^T [\mathbf{I} - (\mathbf{Z} \Sigma \mathbf{Z}^T + \sigma^2 \mathbf{I})^{-1} \mathbf{Z} \Sigma \mathbf{Z}^T] (\mathbf{y} - \mathbf{X} \boldsymbol{\beta}) \\
  &=& \sigma^{-2} \Sigma \mathbf{Z}^T [(\mathbf{Z} \Sigma \mathbf{Z}^T + \sigma^2 \mathbf{I})^{-1} (\mathbf{Z} \Sigma \mathbf{Z}^T + \sigma^2 \mathbf{I}) - (\mathbf{Z} \Sigma \mathbf{Z}^T + \sigma^2 \mathbf{I})^{-1} \mathbf{Z} \Sigma \mathbf{Z}^T] (\mathbf{y} - \mathbf{X} \boldsymbol{\beta}) \\
  &=& \Sigma \mathbf{Z}^T (\mathbf{Z} \Sigma \mathbf{Z}^T + \sigma^2 \mathbf{I})^{-1} (\mathbf{y} - \mathbf{X} \boldsymbol{\beta}).
\end{eqnarray*}
$$
So 
$$
\begin{eqnarray*}
f(\boldsymbol{\gamma} \mid \mathbf{y}) &\propto& \exp\left(-\frac{1}{2}\left[\boldsymbol{\gamma}^T\left(\sigma^{-2}\mathbf{Z}^T\mathbf{Z}+\boldsymbol{\Sigma}^{-1}\right)\boldsymbol{\gamma}-2(\boldsymbol{\gamma}^T(\sigma^{-2} \mathbf{Z}^T \mathbf{Z} + \boldsymbol{\Sigma}^{-1}) (\boldsymbol{\Sigma} \mathbf{Z}^T (\mathbf{Z} \boldsymbol{\Sigma} \mathbf{Z}^T + \sigma^2 I)^{-1} (\mathbf{y} - \mathbf{X} \boldsymbol{\beta})) \right]\right)\\
&=& \exp\left(-\frac{1}{2}\left(\sigma^{-2}\mathbf{Z}^T\mathbf{Z}+\boldsymbol{\Sigma}^{-1}\right)\left[\boldsymbol{\gamma}^T\boldsymbol{\gamma}-2(\boldsymbol{\gamma}^T (\boldsymbol{\Sigma} \mathbf{Z}^T (\mathbf{Z} \boldsymbol{\Sigma} \mathbf{Z}^T + \sigma^2 I)^{-1} (\mathbf{y} - \mathbf{X} \boldsymbol{\beta})) \right]\right)
\end{eqnarray*}
$$
So 
$$
\mathbb{E} (\boldsymbol{\gamma} \mid \mathbf{y}) = \boldsymbol{\Sigma} \mathbf{Z}^T (\mathbf{Z} \boldsymbol{\Sigma} \mathbf{Z}^T + \sigma^2 \mathbf{I})^{-1} (\mathbf{y} - \mathbf{X} \boldsymbol{\beta})
$$

2. For the balanced one-way ANOVA random effects model, show that the posterior mean of random effects is always a constant (less than 1) multiplying the corresponding fixed effects estimate.

**Answer:**
For the balanced one-way ANOVA random effects model, we know
$$
\begin{eqnarray*}
\mathbf{y} = \mathbf{1}_{na} \mu + \begin{pmatrix}
\mathbf{1}_{n} & & \\
& \vdots & \\
& & \mathbf{1}_{n}
\end{pmatrix} \boldsymbol{\gamma} + \boldsymbol{\epsilon}\\
\text{So Z here is } \begin{pmatrix}
\mathbf{1}_{n} & & \\
& \vdots & \\
& & \mathbf{1}_{n}
\end{pmatrix}
\end{eqnarray*}
$$
So the posterior mean of random effects is
$$
\begin{eqnarray*}
\mathbb{E} (\boldsymbol{\gamma} \mid \mathbf{y}) &=& \boldsymbol{\Sigma} \mathbf{Z}^T (\mathbf{Z} \boldsymbol{\Sigma} \mathbf{Z}^T + \sigma^2 \mathbf{I})^{-1} (\mathbf{y} - \mathbf{X} \boldsymbol{\beta})\\
&=& \sigma_{\epsilon}^{-2} (\sigma_{\epsilon}^{-2} \mathbf{Z}^T \mathbf{Z} + \Sigma_{\epsilon}^{-1})^{-1 } \mathbf{Z}^T (\mathbf{y} - \mathbf{X} \boldsymbol{\beta}) \\
\mathbf{Z}^T \mathbf{Z} &=& n \mathbf{I}_a, \quad \Sigma_{\epsilon} = \sigma_{\alpha}^2 \mathbf{I}_a\\
\mathbb{E} (\boldsymbol{\gamma} \mid \mathbf{y})&=& \begin{pmatrix}
\frac{1}{n + (\sigma_{\epsilon}/\sigma_{\alpha})^2} & & \\
& \vdots & \\
& & \frac{1}{n + (\sigma_{\epsilon}/\sigma_{\alpha})^2}
\end{pmatrix} \begin{pmatrix}
 n\bar{y}_{1\cdot} - n\hat \mu \\
\vdots \\
n\bar{y}_{a\cdot} - n\hat \mu
\end{pmatrix} \\
&=&\frac{n}{n + (\sigma_{\epsilon}/\sigma_{\alpha})^2}\begin{pmatrix}
 \bar{y}_{1\cdot} - \hat \mu \\
\vdots \\
\bar{y}_{a\cdot} - \hat \mu
\end{pmatrix} \\
&=& \frac{1}{1 + n^{-1}(\sigma_{\epsilon}/\sigma_{\alpha})^2} \begin{pmatrix}
\hat \alpha_1 \\
\vdots \\
\hat \alpha_a
\end{pmatrix}.
\end{eqnarray*}
$$
Since $n^{-1}(\sigma_{\epsilon}/\sigma_{\alpha})^2>0$, the posterior mean of random effects is always a constant (less than 1) multiplying the corresponding fixed effects estimate.



## Q3. ELMR Exercise 11.1 (p251)

The ratdrink data consist of five weekly measurements of body weight for 27 rats. The first 10 rats are on a control treatment while 7 rats have thyroxine added to their drinking water. Ten rats have thiouracil added to their water.

### Q3.1
(a) Plot the data showing how weight increases with age on a single panel, taking care to distinguish the three treatment groups. Now create a three-panel plot, one for each group. Discuss what can be seen.

**Answer:**
```{r}
library(ggplot2)
library(gridExtra)
library(tidyverse)
ggplot(data = ratdrink, aes(x = weeks, y = wt, color = treat)) +
  geom_line() +
  facet_wrap(~subject) +
  labs(x = "Week", y = "Weight")
```
From this plot, we can see that the weight increases with age for all three treatment groups.

```{r}
ggplot(data = ratdrink, aes(x = weeks, y = wt, group = subject, 
                            color = treat)) +
  geom_line() +
  facet_wrap(~treat) +
  labs(x = "Week", y = "Weight")
```
Control rats and thyroxine treated rats have similar weight gain over time, while thiouracil treated rats have a lower weight gain over time.

### Q3.2
(b) Fit a linear longitudinal model that allows for a random slope and intercept for each rat. Each group should have a different mean line. Give interpretation for the following estimates:

i. The fixed effect intercept term.\
ii. The interaction between thiouracil and week.\
iii. The intercept random effect SD.

**Answer:**
```{r}
library(lme4)
m1 <- lmer(wt ~ weeks * treat + (weeks | subject), data = ratdrink)
summary(m1)
```
+ The fixed effect intercept term: The estimated average weight of the control group at week 0 is 52.88.

+ The interaction between thiouracil and week: For every one week increase, the estimated average weight gain for the thiouracil group is -9.37 weights lower than that for the control group.

+ The intercept random effect SD: The standard deviation between subjects within the same treatment group is 5.7.

### Q3.3
(c) Check whether there is a significant treatment effect.

**Answer:**
```{r}
library(pbkrtest)
m2 <- lmer(wt ~ weeks + (weeks | subject), data = ratdrink)
KRmodcomp(m1, m2)
```
The p-value is 0.0001215, which is less than 0.05. Therefore, there is a significant treatment effect.

### Q3.4
(d) Construct diagnostic plots showing the residuals against the fitted values and a QQ plot of the residuals. Interpret.

**Answer:**
```{r}
plot(resid(m1) ~ fitted(m1), xlab = "Fitted", ylab = "Residuals")
abline(h=0)
```
The residuals are randomly scattered around the 0 line and there is no special pattern, indicating that the linearity and constant variance assumptions are met.

```{r}
qqnorm(resid(m1), main = "")
```
The residuals are close to a straight line, indicating that the normality assumption is met.

### Q3.5
(e) Construct confidence intervals for the parameters of the model. Which random effect terms may not be significant? Is the thyroxine group significantly different from the control group?

**Answer:**
```{r}
confint(m1, method = "boot")
```
`sig01` is the variance of the random intercepts, `sig02` is the covariance between the random intercepts and slopes, and `sig03` is the variance of the random slopes. Both random intercepts and slopes are significant because their intervals do not include 0. The thyroxine group is not significantly different from the control group because the confidence interval includes 0.

## Q4. ELMR Exercise 13.1 (p295)
The ohio data concern 536 children from Steubenville, Ohio and were taken as part of a study on the effects of air pollution. Children were in the study for 4 years from ages 7 to 10. The response was whether they wheezed or not. The variables are:

`resp` an indicator of wheeze status (1 = yes, 0 = no)

`id` an identifier for the child

`age` 7 yrs = −2, 8 yrs = −1, 9 yrs =0, 10 yrs = 1

`smoke` an indicator of maternal smoking at the first year of the study (1=smoker, 0 = nonsmoker)

### Q4.1
(a) Do any of the mothers in the study change their smoking status during the period of observation?

**Answer:**
```{r}
ohio |>
  group_by(id) |>
  summarise(smoke_change = n_distinct(smoke))
sum(ohio$smoke_change > 1)
```
There are 0 mothers who changed their smoking status during the period of observation because the number of distinct smoking statuses is equal to 1 for all mothers, which means they did not change their smoking status.

### Q4.2
(b) Construct a table that shows proportion of children who wheeze for 0, 1, 2, 3 or 4 years broken down by maternal smoking status.

**Answer:**
```{r}
table_data <- ohio |>
  group_by(id, smoke) |>
  summarise(years = sum(resp), .groups = "drop")

prop.table(table(table_data$years, table_data$smoke), 2)
```

### Q4.3
(c) Make plot which shows how the proportion of children wheezing changes by age with a separate line for smoking and nonsmoking mothers.

**Answer:**
```{r}
library(ggplot2)
ohio |>
  count(age, smoke, resp) |>
  group_by(age, smoke) |>
  mutate(prop = prop.table(n)) |>
  filter(resp == 1) |>
  mutate(age = factor(age, levels = c(-2, -1, 0, 1), 
         labels = c(7, 8, 9, 10)), 
         smoke = factor(smoke, levels = c(0, 1),
                        labels = c("Nonsmoking", "Smoking"))) |>
  ungroup() |>
  ggplot(aes(x = age, y = prop, color = smoke, group = smoke)) +
  geom_line() +
  labs(x = "Age", y = "Proportion of children wheezing")
```

### Q4.4
(d) Group the data by child to count the total (out of four) years of wheezing. Fit a binomial GLM to this response to check for a maternal smoking effect. Does this prove there is a smoking effect or could there be another plausible explanation?

**Answer:**
```{r}
ohio1 <- ohio |>
  group_by(id) |>
  summarise(whez_years = sum(resp)) |>
  mutate(non_whez_years = 4 - whez_years) |>
  mutate(smoke = ohio$smoke[match(id, ohio$id)], .after = non_whez_years)
glm(cbind(whez_years, non_whez_years) ~ smoke, data = ohio1, 
    family = binomial) |>
  summary()
```
The p-value of `smoke` is $0.0277 <0.05$, which indicates that there is a maternal smoking effect. However, there could be another plausible explanation because there could be something confounding the results.

### Q4.5
(e) Fit a model for each individual response using a GLMM fit using penalized quasi-likelihood. Describe the effects of age and maternal smoking. How do the odds of wheezing change numerically over time?

**Answer:**
```{r}
library(MASS)
pqlmod <- glmmPQL(resp ~ age + smoke, random = ~ 1 | id, family = binomial, data = ohio)
summary(pqlmod)
```
For every one year increase in age, the odds of wheezing decreases by $1- e^{-0.182} = 16.6\%$, controlling for smoking condition. The odds of wheezing for children of smoking mothers are $e^{0.325} = 1.384$ times higher than that for children of nonsmoking mothers, controlling for age. The odds of wheezing decreases by $1- e^{-0.182} = 16.6\%$ over time. 

### Q4.6
(f) Now fit the same model but using adaptive Gaussian-Hermit quadrature. Compare to the previous model fit.

**Answer:**
```{r}
library(lme4)
ghmod <- glmer(resp ~ age + smoke + (1 | id), nAGQ = 25, family = binomial, 
               data = ohio)
summary(ghmod)
```
The coefficients of `age` and `smoke` are similar to the previous model fit. For every one year increase in age, the odds of wheezing decreases by $1- e^{-0.176} = 16.1\%$, controlling for smoking condition. The odds of wheezing for children of smoking mothers are $e^{0.399} = 1.49$ times higher than that for children of nonsmoking mothers, controlling for age. The odds of wheezing decreases by $1- e^{-0.176} = 16.1\%$ over time.

### Q4.7
(g) Use INLA to fit the same model. What does this model say about the effect of age and maternal smoking (optional)?

**Answer:**
```{r}
library(INLA)
formula <- resp ~ age + smoke + f(id, model = "iid")
inlamod <- inla(formula, family = "binomial", data = ohio)
summary(inlamod)
```
For every one year increase in age, the odds of wheezing decreases by $1- e^{-0.173} = 15.9\%$, controlling for smoking condition. The odds of wheezing for children of smoking mothers are $e^{0.385} = 1.47$ times higher than that for children of nonsmoking mothers, controlling for age.

### Q4.8
(h) Use STAN to fit the same model. Check the MCMC diagnostics and again discuss the age and maternal smoking effects (optional).


### Q4.9
(i) Fit the model using GEE. Use an autoregressive rather than exchangeable error structure. Compare the results to the previous model fits. In your model, what indicates that a child who already wheezes is likely to continue to wheeze?

**Answer:**
```{r}
library(geepack)
geemod <- geeglm(resp ~ age + smoke, id = id, corstr = "ar1", data = ohio, 
                 family =binomial(link = "logit"))
summary(geemod)
```
For every one year increase in age, the odds of wheezing decreases by $1- e^{-0.115} = 10.86\%$, controlling for smoking condition. The odds of wheezing for children of smoking mothers are $e^{0.233} = 1.262$ times higher than that for children of nonsmoking mothers, controlling for age. The odds of wheezing decreases by $1- e^{-0.115} = 10.86\%$ over time. The correlation coefficient of the autoregressive error structure is $0.5$, which indicates that a child who already wheezes is likely to continue to wheeze.

### Q4.10
(j) What is your overall conclusion regarding the effect of age and maternal smoking? Can we trust the GLM result or are the GLMM models preferable?

**Answer:**
The `age` has a significant effect on the odds of wheezing while `smoke` does not have a significant effect on the odds of wheezing. The GLMM models are preferable because they consider the dependency between the repeated measurements of the same child.

