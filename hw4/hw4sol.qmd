---
title: "Biostat 200C Homework 4"
subtitle: Due May 24  @ 11:59PM
author: "Ziheng Zhang_606300061"
date: today
format:
  html:
    theme: cosmo
    embed-resources: true
    number-sections: true
    toc: true
    toc-depth: 4
    toc-location: left
    code-fold: false
engine: knitr
knitr:
  opts_chunk: 
    fig.align: 'center'
    # fig.width: 6
    # fig.height: 4
    message: FALSE
    cache: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = FALSE)
```

## Q1. ELMR Excercise 7.5 (p150)
The debt data arise from a large postal survey on the psychology of debt. The frequency of credit card use is a three-level factor ranging from never, through occasionally to regularly.

### Q1.1
(a) Declare the response as an ordered factor and make a plot showing the relationship to `prodebt`. Comment on the plot. Use a table or plot to display the relationship between the response and the income group.

```{r}
library(faraway)
library(tidyverse)
library(ggplot2)

debt1 <- debt
debt1$ccarduse <- ordered(debt1$ccarduse, levels = c(1, 2, 3))
ggplot(drop_na(debt1, c(ccarduse, prodebt)), aes(x = ccarduse, y = prodebt)) + 
  geom_boxplot()
```
**Answer:** From the boxplot, we can see that the variability of `prodebt` is almost the same among all different levels of credit card use. The median of `prodebt` is increasing with the level of credit card use. 
```{r}
debt1 |>
  drop_na(c(ccarduse, incomegp)) |>
  count(ccarduse, incomegp) |>
  group_by(incomegp) |>
  mutate(prop = prop.table(n)) |>
  ggplot() +
  geom_line(mapping = aes(x = incomegp, y = prop, group = ccarduse, color = ccarduse)) + 
  labs(x = "Income Level", y = "Proportion")
```
**Answer:** As the use of credit card increases, the proportion of people in the lowest income group decreases, while the proportion of people in the highest income group increases. The proportion of people in the middle income group is relatively stable.

### Q1.2
(b) Fit a proportional odds model for credit card use with all the other variables as predictors. What are the two most significant predictors (largest t-values) and what is their qualitative effect on the response? What is the least significant predictor?

```{r}
library(MASS)
fit1 <- polr(ccarduse ~ ., data = debt1)
summary(fit1)
```
**Answer:** The two most significant predictors are `bankacc` (does the respondent have a bank account?) and `incomegp` (income group (1=lowest, 5=highest)). The least significant predictor is `house` (security of housing tenure (1=rent, 2=mortgage, 3=owned outright)). The coefficient of `bankacc` is $2.1027$, which means the odds of moving up in one category of frequency of credit card use is $e^{2.1027} = 8.19$ times higher for people with bank account than those without bank account, controlling for all other predictors. The coefficient of `incomegp` is $0.47131$, which means for every one level increase of income, the odds of moving up in one category of frequency of credit card use is estimated to increase by $e^{0.47131} -1 = 60\%$, controlling for all other predictors.

### Q1.3
(c) Fit a proportional odds model using only the least significant predictor from the previous model. What is the significance of this predictor in this small model? Are the conclusions regarding this predictor contradictory for the two models?

```{r}
fit2 <- polr(ccarduse ~ house, data = debt1)
summary(fit2)
```
**Answer:** The t value of the predictor `house` is $3.895$ in this small model, which is significant in this small model. The two models have different conclusions for this predictors. The conclusion regarding this predictor is contradictory for the two models.

### Q1.4
(d) Use stepwise AIC to select a smaller model than the full set of predictors. You will need to handle the missing values carefully. Report on the qualitative effect of the predictors in your chosen model. Can we conclude that the predictors that were dropped from the model have no relation to the response?

```{r}
debt2 <- na.omit(debt1)
fit3 <- polr(ccarduse ~ ., data = debt2)
fit3_aic <- step(fit3, trace = F)
summary(fit3_aic)
```
**Answer:** The selected model is `ccarduse ~ bankacc + incomegp + bsocacc + agegp + cigbuy + prodebt` and we drop six predictors.\
The coefficient of `bankacc` is $2.0816$, which means the odds of moving up in one category of frequency of credit card use is $e^{2.0816} = 8.02$ times higher for people with bank account than those without bank account, controlling for all other predictors.\
The coefficient of `incomegp` is $0.4589$, which means for every one level increase of income, the odds of moving up in one category of frequency of credit card use is estimated to increase by $e^{0.4589} -1 = 58\%$, controlling for all other predictors.\
The coefficient of `bsocacc` is $0.5048$, which means the odds of moving up in one category of frequency of credit card use is $e^{0.5048} = 1.66$ times higher for people with a building society account than those without a building society account, controlling for all other predictors.\
The coefficient of `agegp` is $0.2696$, which means for every one level increase of age group, the odds of moving up in one category of frequency of credit card use is estimated to increase by $e^{0.2696} - 1 = 31\%$, controlling for all other predictors.\
The coefficient of `cigbuy` is $-0.7677$, which means the odds of moving up in one category of frequency of credit card use is $e^{-0.7677} = 0.464$ times lower for people who buy cigarettes than those who do not buy cigarettes, controlling for all other predictors.\
The coefficient of `prodebt` is $0.5635$, which means for every one score increase of `prodebt`, the odds of moving up in one category of frequency of credit card use is estimated to increase by $e^{0.5635} -1 = 76\%$, controlling for all other predictors.\
We cannot conclude that the predictors that were dropped from the model have no relation to the response because AIC method drops some predictors based on AIC values, we may drop some predictors whose adjusted effects are not significant but marginal effects may be significant.

### Q1.5
(e) Compute the median values of the predictors in your selected model. At these median values, contrast the predicted outcome probabilities for both smokers and nonsmokers.

```{r}
debt2 |> dplyr::select(incomegp, agegp, bankacc, bsocacc, cigbuy, prodebt) |>
  apply(2, median)

predict(fit3_aic, newdata = data.frame(incomegp = 3, agegp = 2, bankacc = 1, 
                                       bsocacc = 1, cigbuy = 0, prodebt = 3.18),
        type = "probs")
predict(fit3_aic, newdata = data.frame(incomegp = 3, agegp = 2, bankacc = 1, 
                                       bsocacc = 1, cigbuy = 1, prodebt = 3.18),
        type = "probs")
```
**Answer:** The predicted outcome probabilities for both smokers and nonsmokers are as follows:

- For `never` use credit card, the probabilities are $0.426$ for nonsmokers and $0.615$ for smokers.\
- For `rarely` use credit card, the probabilities are $0.325$ for nonsmokers and $0.251$ for smokers.\
- For `regularly` use credit card, the probabilities are $0.250$ for nonsmokers and $0.133$ for smokers.

### Q1.6
(f) Fit a proportional hazards model to the same set of predictors and recompute the two sets of probabilities from the previous question. Does it make a difference to use this type of model?

```{r}
fit4 <- polr(ccarduse ~ incomegp + agegp + bankacc + bsocacc + cigbuy + prodebt,
             data = debt2, method = "cloglog")
predict(fit4, newdata = data.frame(incomegp = 3, agegp = 2, bankacc = 1, 
                                       bsocacc = 1, cigbuy = 0, prodebt = 3.18),
        type = "probs")
predict(fit4, newdata = data.frame(incomegp = 3, agegp = 2, bankacc = 1, 
                                       bsocacc = 1, cigbuy = 1, prodebt = 3.18),
        type = "probs")
```
**Answer:** The predicted outcome probabilities for both smokers and nonsmokers only differ slightly from the previous model. It does not make a difference to use this type of model.

## Q2. Moments of exponential family distributions

Show that the exponential family distributions have moments
$$
\begin{eqnarray*}
  \mathbb{E}Y &=& \mu = b'(\theta) \\
  \operatorname{Var}Y &=& \sigma^2 = b''(\theta) a(\phi).
\end{eqnarray*}
$$

**Answer:**
In GLM, the distribution of $Y$ is from the exponential familty of distributions of form
$$
  f(y \mid \theta, \phi) = \exp \left[ \frac{y \theta - b(\theta)}{a(\phi)} + c(y, \phi) \right].
$$
We know that 
$$
  \int f(y) dy = \int \exp \left[ \frac{y \theta - b(\theta)}{a(\phi)} + c(y, \phi) \right] dy =1
$$
So we have
$$
\frac{\partial}{\partial \theta} \int f(y) dy = \int \frac{\partial}{\partial \theta} f(y) dy = 0
$$
Since 
$$
\frac{\partial}{\partial \theta} f(y) = \frac{y - b'(\theta)}{a(\phi)} f(y)
$$
So we have
$$
\begin{eqnarray*}
  0 &=& \int \frac{y - b'(\theta)}{a(\phi)} f(y) dy \\
  &=& \int \frac{y}{a(\phi)} f(y) dy - \int \frac{b'(\theta)}{a(\phi)} f(y) dy \\
  &=& \int y f(y) dy - b'(\theta) \int f(y) dy \\
  &=& \int y f(y) dy - b'(\theta) \\
  &=& \mathbb{E}Y - b'(\theta) = 0 \\
\mathbb{E}Y &=& b'(\theta)
\end{eqnarray*}
$$
Similarly, for variance, we have
$$
\begin{eqnarray*}
  \frac{\partial^2}{\partial \theta^2} \int f(y) dy &=& \int \frac{\partial^2}{\partial \theta^2} f(y) dy = 0 \\
  \frac{\partial^2}{\partial \theta^2} f(y) &=& \left(\frac{y - b'(\theta)}{a(\phi)}\right)^2 f(y)-\frac{b''(\theta)}{a(\phi)} f(y) \\
  0 &=& \int \left(\frac{y - b'(\theta)}{a(\phi)}\right)^2 f(y) dy - \int \frac{b''(\theta)}{a(\phi)} f(y) dy \\
 &=& \int (y - b'(\theta))^2 f(y) dy - \int b''(\theta)a(\phi) f(y) dy \\
 b''(\theta)a(\phi) &=& \int y^2 f(y) dy - 2b'(\theta) \int y f(y) dy + b'(\theta)^2 \int f(y) dy \\
 &=& \mathbb{E}Y^2 - 2b'(\theta) \mathbb{E}Y + b'(\theta)^2 \\
 &=& \mathbb{E}Y^2 - 2b'(\theta)^2 + b'(\theta)^2 \\
 &=& \mathbb{E}Y^2 - b'(\theta)^2 \\
 &=& \mathbb{E}Y^2-\mathbb{E}^2Y\\
 \operatorname{Var}Y &=& b''(\theta) a(\phi)
\end{eqnarray*}
$$

## Q3. Score and information matrix of GLM

Derive the gradient (score), negative Hessian, and Fisher information matrix (expected negative Hessian) of GLM.

**Answer:**
The log-likelihood function of GLM is
$$
\begin{eqnarray*}
  \ell(\boldsymbol{\beta}) &=& \sum_{i=1}^n \left[ \frac{y_i \theta_i - b(\theta_i)}{a(\phi)} + c(y_i, \phi) \right] \\
\end{eqnarray*}
$$
Since we have $\mu_i = b'(\theta_i)$, $\eta = g(\mu)$ and $\eta = \mathbf{x}^T \boldsymbol{\beta}$, then we have 
$$
\begin{eqnarray*}
\frac{\partial \theta_i}{\partial \mu_i} &=& \frac{1}{b''(\theta_i)}\\
\frac{\partial \mu_i}{\partial \eta_i} &=& \mu_i'(\eta_i)\\
\frac{\partial \eta_i}{\partial \boldsymbol{\beta}} &=& \mathbf{x}_i
\end{eqnarray*}
$$
So the gradient (score) is
$$
\begin{eqnarray*}
  \nabla \ell(\boldsymbol{\beta}) &=& \frac{\partial \ell(\boldsymbol{\beta})}{\partial \boldsymbol{\beta}}
  = \sum_{i=1}^n  \frac{\partial \ell(\boldsymbol{\beta})}{\partial \theta_i} \frac{\partial \theta_i}{\partial \mu_i} \frac{\partial \mu_i}{\partial \eta_i} \frac{\partial \eta_i}{\partial \boldsymbol{\beta}} \\
  &=& \sum_{i=1}^n \left[ \frac{y_i - b'(\theta_i)}{a(\phi)} \frac{\mu_i'(\eta_i)}{b''(\theta_i)} \mathbf{x}_i \right] \\
  &=& \sum_{i=1}^n \left[ \frac{(y_i - \mu_i)\mu_i'(\eta_i)}{\sigma_i^2} \mathbf{x}_i \right], \quad \text{since } \sigma_i^2 = a(\phi) b''(\theta_i)
\end{eqnarray*}
$$
So the negative Hessian is
$$
\begin{eqnarray*}
-\nabla^2 \ell(\boldsymbol{\beta}) &=& -\sum_{i=1}^n \frac{\mu_i'(\eta_i)}{\sigma_i^2} \mathbf{x}_i \frac{\partial (y_i - \mu_i)}{\partial \eta_i} \frac{\partial \eta_i}{\partial \boldsymbol{\beta}} -\sum_{i=1}^n  \frac{(y_i - \mu_i)}{\sigma_i^2} \frac{\partial \mu_i'(\eta_i)}{\partial \eta_i} \frac{\partial \eta_i}{\partial \boldsymbol{\beta}}\mathbf{x}_i \\
&& -\sum_{i=1}^n (y_i - \mu_i)\mu_i'(\eta_i) \mathbf{x}_i \frac{\partial}{\partial \mu_i}(\frac{1}{\sigma_i^2}) \frac{\partial \mu_i}{\partial \eta_i} \frac{\partial \eta_i}{\partial \boldsymbol{\beta}} \\
&=& \sum_{i=1}^n \frac{[\mu_i'(\eta_i)]^2}{\sigma_i^2} \mathbf{x}_i \mathbf{x}_i^T - \sum_{i=1}^n \frac{(y_i - \mu_i) \mu_i''(\eta_i)}{\sigma_i^2} \mathbf{x}_i \mathbf{x}_i^T \\
  & & + \sum_{i=1}^n \frac{(y_i - \mu_i) [\mu_i'(\eta_i)]^2 (d \sigma_i^{2} / d\mu_i)}{\sigma_i^4} \mathbf{x}_i \mathbf{x}_i^T
\end{eqnarray*}
$$
So the Fisher information matrix is
$$
 \mathbb{E} [- \nabla^2 \ell(\boldsymbol{\beta})] = \sum_{i=1}^n \frac{[\mu_i'(\eta_i)]^2}{\sigma_i^2} \mathbf{x}_i \mathbf{x}_i^T , \\
 \text{since } \mathbb{E} (y_i - \mu_i) = 0 \text{ and the second and third terms become zero.}
$$

## Q4. ELMR Exercise 8.1 (p171)
Data is generated from the exponential distribution with density $f(y) = \lambda exp(−\lambda y)$ where $\lambda$, $y > 0$.

### Q4.1
(a) Identify the specific form of $\theta$, $\phi$, $a()$, $b()$ and $c()$ for the exponential distribution.
$$
\begin{eqnarray*}
f(y \mid \theta, \phi) &=& \exp \left[ \frac{y \theta - b(\theta)}{a(\phi)} + c(y, \phi) \right]\\
&=& \exp \left[-\lambda y + \log \lambda \right] 
\end{eqnarray*}
$$
So we have $\theta = -\lambda$, $a(\phi) = \phi = 1$, $b(\theta) = -\log (-\theta)$ and $c(y, \phi) = 0$.

### Q4.2
(b) What is the canonical link and variance function for a GLM with a response following the exponential distribution?

**Answer:** The canonical link is $g(\mu)$ s.t. $\eta = g(\mu) = \theta$. And we have $\mu = b'(\theta) = -\frac{1}{\theta}$. So the canonical link is $g(\mu) = -\frac{1}{\mu}$. The $b''(\theta) = \frac{1}{\theta^2}$, so the variance function is $\mu^2$.

### Q4.3
(c) Identify a practical difficulty that may arise when using the canonical link in this instance.

**Answer:** $\mu$ is always positive and $\eta = g(\mu) = -\frac{1}{\mu}$. So in this case, the linear predictor $\eta$ will always be negative and then it is not practical to use the canonical link in this instance.

### Q4.4
(d) When comparing nested models in this case, should an F or $\chi^2$ test be used? Explain.

**Answer:** Here we have dispersion parameter $\phi = 1$ so it is known. So we should use $\chi^2$ test to compare nested models in this case. And we will use F test when the dispersion parameter is unknown and estimated.

### Q4.5
(e) Express the deviance in this case in terms of the responses $y_i$ and the fitted values $\hat{\mu}_i$.

We can use likelihood ratio test. 
$$
2 \log \frac{L_{\Omega}}{L_{\omega}},
$$
where $\Omega$ is the full/saturated model (same number of parameters as observations) and $\omega$ is the smaller model. So the deviance is 
$$
\begin{eqnarray*}
D &=& 2 \sum \log \frac{f(y_i \mid y_i)}{f(y_i \mid \hat{\mu}_i)} \\ 
&=& 2 \sum \left[ -\frac{1}{y_i} y_i + \log \frac{1}{y_i} + \frac{1}{\hat{\mu}_i} y_i - \log \frac{1}{\hat{\mu}_i} \right] \\
&=& 2 \sum(\frac{y_i-\hat{\mu}_i}{\hat{\mu}_i} - \log\frac{y_i}{\hat{\mu}_i})
\end{eqnarray*}
$$

## Q5. ELMR Exercise 8.4 (p172)
Consider the Galápagos data and model analyzed in this chapter. The purpose of this question is to reproduce the details of the GLM fitting of this data.

### Q5.1
(a) Fit a Poisson model to the species response with the five geographic variables as predictors. Do not use the endemics variable. Report the values of the coefficients and the deviance.
```{r}
model1 <- glm(Species ~ Area + Elevation + Nearest + Scruz + Adjacent, 
              data = gala, family = poisson)
summary(model1)
```
**Answer:** The coefficient of `Area` is $ -0.0005799$, `Elevation` is $0.003541$, `Nearest` is $0.008826$, `Scruz` is $-0.005709$ and `Adjacent` is $-0.000663$. The residual deviance is $ 716.85$.

### Q5.2
(b) For a Poisson GLM, derive $\eta$, $\frac{d\eta}{d\mu}$, $V(\mu)$ and the weights to be used in an iteratively fit GLM. What is the form of the adjusted dependent variable here?
$$
\begin{eqnarray*}
\eta &=& \log(\mu) = \beta_0 + \beta_1 x_1 + \cdots + \beta_p x_p, \\
\frac{d\eta}{d\mu} &=& \frac{1}{\mu}, \\
V(\mu) &=& \mu, \\
\text{Next, we need to find the weights:} \\
\mathbb{P}(Y_i = y_i) &=& e^{-\mu_i} \frac{\mu_i^{y_i}}{y_i!} \\
\ell(\boldsymbol{\beta}) &=& \sum_{i=1}^n y_i \log \mu_i - \mu_i - \log y_i! \\
&=& \sum_{i=1}^n \left[ y_i \mathbf{x}_i^T \boldsymbol{\beta} - e^{\mathbf{x}_i^T \boldsymbol{\beta}} - \log y_i! \right] \\
\nabla_{\beta} \ell(\boldsymbol{\beta}) &=& \sum_{i=1}^n \left[y_{i} \mathbf{x}_{i} - e^{\mathbf{x}_{i}^T \boldsymbol{\beta}} \mathbf{x}_{i} \right] \\
-H_{\beta} &=& -\nabla^{2}_{\beta} \ell(\boldsymbol{\beta}) \\
&=& \sum_{i=1}^n \left[e^{\mathbf{x}_{i}^{T} \boldsymbol{\beta}} \mathbf{x}_{i} \mathbf{x}_{i}^{T} \right] =  \mathbf{X}^T \mathbf{W} \mathbf{X} \\
\text{So the weight is:}\\
w &=& e^{\mathbf{x}^T \boldsymbol{\beta}} = e^{\eta} = \mu
\end{eqnarray*}
$$
The adjusted dependent variable is $z^{(t)} = \eta^{(t)} + \frac{y-\hat{\mu}^{(t)}}{\hat{\mu}^{(t)}}$.

### Q5.3
(c) Using the observed response as initial values, compute the first stage of the iteration, stopping after the first linear model fit. Compare the coefficients of this linear model to those found in the GLM fit. How close are they?
```{r}
y <- gala$Species
mu <- gala$Species
eta <- log(mu)
z <- eta + (y-mu)/mu
w <- mu
model2 <- lm(z ~ . - Endemics - Species, weights = w, gala)
coef(model2)
```
**Answer:** The intercept of this linear model and the coefficients of `Area`, `Elevation` and `Adjacent` in this linear model are close to those found in the GLM fit.

### Q5.4
(d) Continue the iteration to get the next $\eta$ and $\mu$. Use this to compute the current value of the deviance. How close is this to the deviance from the GLM?
```{r}
eta <- model2$fitted.values
mu <- exp(eta)
z <- eta + (y-mu)/mu
w <- mu
model2 <- lm(z ~ . - Endemics - Species, weights = w, gala)
deviance(model2)
```
**Answer:** The deviance from this linear model is 570.9648, which is not close to the deviance from the GLM.

### Q5.5
(e) Compute one more iteration of the GLM fit, reporting the next calculation of the coefficients and deviance. How close are these to target now?
```{r}
eta <- model2$fitted.values
mu <- exp(eta)
z <- eta + (y-mu)/mu
w <- mu
model2 <- lm(z ~ . - Endemics - Species, weights = w, gala)
coef(model2)
deviance(model2)
```
**Answer:** The coefficients of this linear model are all close to those found in the GLM fit. The deviance from this linear model is 725.5103, which is close to the deviance from the GLM.

### Q5.6
(f) Repeat these iterations a few more times, computing the deviance in each time. Stop when the deviance does not change much. Compare your final estimated coefficients to that produced by the GLM fit.
```{r}
for (i in 1:5) {
  eta <- model2$fitted.values
  mu <- exp(eta)
  z <- eta + (y-mu)/mu
  w <- mu
  model2 <- lm(z ~ . - Endemics - Species, weights = w, gala)
  deviance(model2)
  cat(i, deviance(model2), "\n")
}
coef(model2)
```
**Answer:** After five more iterations, the deviance does not change and it is 761.9792, which is close to the deviance from the GLM. The final estimated coefficients are almost the same as that produced by the GLM fit.

### Q5.7
(g) Use your final iterated linear model fit to produce standard errors for the coefficients. How close are these to that produced by the direct GLM fit?
```{r}
xm <- model.matrix(model2)
wm <- diag(w)
sqrt(diag(solve(t(xm) %*% wm %*% xm)))
```
**Answer:** The standard errors for the coefficients produced by the iterated linear model fit are almost the same as that produced by the direct GLM fit.

## Q6. ELMR Exercise 8.5 (p172)
Again using the Galápagos data, fit a Poisson model to the species response with the five geographic variables as predictors. Do not use the endemics variable. The purpose of this question is to compare six different ways of testing the significance of the elevation predictor, i.e., $H_0$: $\beta_{Elev}$ = 0. In each case, report the p-value.

### Q6.1
(a) Use the z-statistic from the model summary.
```{r}
summary(model1)
```
**Answer:** The p-value of the z-statistic for `Elevation` is $< 2e-16$, which means that the `Elevation` predictor is significant and can contribute to explaining variation in the `Species` response.

### Q6.2
(b) Fit a model without elevation and use the difference in deviances to make the test.
```{r}
model3 <- glm(Species ~ Area + Nearest + Scruz + Adjacent, 
              data = gala, family = poisson)
summary(model3)
anova(model3, model1, test = "Chisq")
```
**Answer:** The p-value of the difference in deviances is $< 2.2e-16$, which means that the `Elevation` predictor is significant and can contribute to explaining variation in the `Species` response.

### Q6.3
(c) Use the Pearson Chi-squared statistic in place of the deviance in the previous test.
```{r}
px2 <- sum(residuals(model1, type = "pearson")^2)
pchisq(px2, model1$df.residual, lower.tail = FALSE)

px2_1 <- sum(residuals(model3, type = "pearson")^2)
pchisq(px2_1, model3$df.residual, lower.tail = FALSE)
```
**Answer:** The p-value of the Pearson Chi-squared statistic in the model with `Elevation` is almost $0$ and the p-value of the Pearson Chi-squared statistic in the model without `Elevation` is $0$. Both models do not fit the data well but the model without `Elevation` fits the data worse. So we should include `Elevation` in the model.

### Q6.4
(d) Fit the Poisson model with a free dispersion parameter as described in Section 5.2. Make the test using the model summary.
```{r}
dp1 <- sum(residuals(model1, type = "pearson")^2) / model1$df.residual
summary(model1, dispersion = dp1)
```
**Answer:** The p-value of the z-statistic for `Elevation` is $< 6.53e-13$, which means that the `Elevation` predictor is significant and can contribute to explaining variation in the `Species` response.

### Q6.5
(e) Use the sandwich estimation method for the standard errors in the original model. Use these to compute z-statistics.
```{r}
library(sandwich)
se_san <- model1 |> 
  vcovHC() |>
  diag() |>
  sqrt()
z <- coef(model1) / se_san
p <- (1 - pnorm(abs(z), 0, 1))*2
p
```
**Answer:** The p-value of the z-statistic for `Elevation` is $0.003 < 0.05$, which means that the `Elevation` predictor is significant and can contribute to explaining variation in the `Species` response.

### Q6.6
(f) Use the robust GLM estimation method and report the test result from the summary.
```{r}
library(robust)
glmRob(Species ~ Area + Nearest + Scruz + Adjacent + Elevation, 
       data = gala, family = poisson) |>
  summary()
```
**Answer:** The p-value of the z-statistic for all the predictors except for `Adjacent` is $0$, which means that the `Elevation` predictor is significant and can contribute to explaining variation in the `Species` response.

### Q6.7
(g) Compare all six results. Pick the best one and justify your choice.

**Answer:** It seems that fitting the Poisson model with a free dispersion parameter is the best method. It can make the variance be equal to the mean and the coefficients from this method is same as the coefficients from the original model. 


